============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l3-experiment
plugins: cov-6.0.0, anyio-4.7.0
collected 49 items

tests/test_consciousness.py EEEEEE                                       [ 12%]
tests/test_environment.py FFs...s                                        [ 26%]
tests/unit/attention/test_attention.py FFFF.                             [ 36%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 44%]
tests/unit/integration/test_cognitive_integration.py EEEE                [ 53%]
tests/unit/integration/test_state_management.py F.F.                     [ 61%]
tests/unit/memory/test_integration.py FFFF                               [ 69%]
tests/unit/memory/test_memory.py EEEEE                                   [ 79%]
tests/unit/memory/test_memory_components.py EEEE                         [ 87%]
tests/unit/state/test_consciousness_state_management.py ...F..           [100%]

==================================== ERRORS ====================================
______ ERROR at setup of TestConsciousnessModel.test_model_initialization ______

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf2970a90>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_forward_pass _______

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf2970100>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
__________ ERROR at setup of TestConsciousnessModel.test_model_config __________

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf29714b0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
___ ERROR at setup of TestConsciousnessModel.test_model_state_initialization ___

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf2971300>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_state_update _______

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf2971780>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
____ ERROR at setup of TestConsciousnessModel.test_model_attention_weights _____

self = <test_consciousness.TestConsciousnessModel object at 0x76bbf29712a0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cross_modal_attention _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x76bbf29f9720>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_modality_specific_processing _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x76bbf29f9960>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_integration_stability _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x76bbf29f9d20>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cognitive_integration _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x76bbf29fa0e0>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
________ ERROR at setup of TestMemoryComponents.test_gru_state_updates _________

self = <test_memory.TestMemoryComponents object at 0x76bbf29f90c0>
hidden_dim = 64

    @pytest.fixture
    def gru_cell(self, hidden_dim):
        """Create GRU cell for testing."""
>       return GRUCell(hidden_dim=hidden_dim).to(self.device)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:50: TypeError
____ ERROR at setup of TestMemoryComponents.test_memory_sequence_processing ____

self = <test_memory.TestMemoryComponents object at 0x76bbf29f8b80>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1,
            input_dim=hidden_dim
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_______ ERROR at setup of TestMemoryComponents.test_context_aware_gating _______

self = <test_memory.TestMemoryComponents object at 0x76bbf29f8cd0>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1,
            input_dim=hidden_dim
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____ ERROR at setup of TestMemoryComponents.test_information_integration ______

self = <test_memory.TestMemoryComponents object at 0x76bbf29f8460>
hidden_dim = 64

    @pytest.fixture
    def info_integration(self, hidden_dim):
        """Create information integration module for testing."""
        return InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=4,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x76bbf29f8460>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_________ ERROR at setup of TestMemoryComponents.test_memory_retention _________

self = <test_memory.TestMemoryComponents object at 0x76bbf291b970>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____________ ERROR at setup of TestGRUCell.test_gru_state_updates _____________

self = <test_memory_components.TestGRUCell object at 0x76bbf28710c0>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
______________ ERROR at setup of TestGRUCell.test_gru_reset_gate _______________

self = <test_memory_components.TestGRUCell object at 0x76bbf28714b0>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
_________ ERROR at setup of TestWorkingMemory.test_sequence_processing _________

self = <test_memory_components.TestWorkingMemory object at 0x76bbf2871510>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
__________ ERROR at setup of TestWorkingMemory.test_memory_retention ___________

self = <test_memory_components.TestWorkingMemory object at 0x76bbf2871ae0>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
=================================== FAILURES ===================================
______________________ EnvironmentTests.test_core_imports ______________________

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
>           import torchvision

tests/test_environment.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164: in <module>
    def meta_nms(dets, scores, iou_threshold):
../../.local/lib/python3.10/site-packages/torch/library.py:795: in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
../../.local/lib/python3.10/site-packages/torch/library.py:184: in _register_fake
    handle = entry.fake_impl.register(func_to_register, source)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch._library.fake_impl.FakeImplHolder object at 0x76bbf29b8370>
func = <function meta_nms at 0x76bbf27149d0>
source = '/home/kasinadhsarma/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164'

    def register(self, func: Callable, source: str) -> RegistrationHandle:
        """Register an fake impl.
    
        Returns a RegistrationHandle that one can use to de-register this
        fake impl.
        """
        if self.kernel is not None:
            raise RuntimeError(
                f"register_fake(...): the operator {self.qualname} "
                f"already has an fake impl registered at "
                f"{self.kernel.source}."
            )
>       if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
E       RuntimeError: operator torchvision::nms does not exist

../../.local/lib/python3.10/site-packages/torch/_library/fake_impl.py:31: RuntimeError
___________________ EnvironmentTests.test_framework_versions ___________________

self = <test_environment.EnvironmentTests testMethod=test_framework_versions>

    def test_framework_versions(self):
        """Verify framework versions"""
        import torch
>       import torchvision

tests/test_environment.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26: in <module>
    def meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function meta_roi_align at 0x76bbf27165f0>

    def wrapper(fn):
>       if torchvision.extension._has_ops():
E       AttributeError: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)

../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18: AttributeError
_______________ TestAttentionMechanisms.test_scaled_dot_product ________________

self = <test_attention.TestAttentionMechanisms object at 0x76bbf29700a0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_scaled_dot_product(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test scaled dot-product attention computation."""
        # Create inputs
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:46: TypeError
_________________ TestAttentionMechanisms.test_attention_mask __________________

self = <test_attention.TestAttentionMechanisms object at 0x76bbf29727a0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_attention_mask(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test attention mask handling."""
        # Create inputs and mask
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:61: TypeError
___________ TestAttentionMechanisms.test_consciousness_broadcasting ____________

self = <test_attention.TestAttentionMechanisms object at 0x76bbf2997c40>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_consciousness_broadcasting(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test consciousness-aware broadcasting."""
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:74: TypeError
__________ TestAttentionMechanisms.test_global_workspace_integration ___________

self = <test_attention.TestAttentionMechanisms object at 0x76bbf2997f40>
batch_size = 2, seq_length = 8, hidden_dim = 128, num_heads = 4

    def test_global_workspace_integration(self, batch_size, seq_length, hidden_dim, num_heads):
        """Test global workspace integration."""
        workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=0.1
        )
    
>       inputs = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:93: TypeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_state_management.TestConsciousnessStateManager object at 0x76bbf29fb370>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4678],
        [0.4775]])
state: tensor([[ 0.2075,  1.1498, -1.6802,  2.0736,  0.3430,  0.2894, -1.1141, -2.4913,
          1.1890,  0.8835,  1.6965,  0.1750,  0.7175,  1.5655, -1.5676,  0.4462,
          0.3661, -0.2812, -0.4124, -0.4769, -0.5760, -1.1616,  2.4230,  0.7418,
          1.5732,  0.3973,  0.4799, -1.0981, -0.7874, -1.6198, -1.2240,  1.3990,
          0.3629, -1.3498,  0.0667, -0.3869,  1.6541, -0.4802,  0.6432,  0.9204,
          0.5140,  0.9032,  2.2852, -0.4410,  1.1671,  0.7003,  1.0036,  0.6227,
          1.3092, -1.3055, -0.2891, -0.1529,  0.5108,  0.1867,  0.2384,  1.5739,
         -2.2392, -0.1516, -0.3696,  0.7432,  0.5666,  0.7863,  0.1317, -0.3194],
        [ 0.6961, -0.7164, -0.5508, -0.2616, -0.2286, -1.1550, -0.0462, -0.5361,
          1.7602, -0.0761,  0.5294,  0.8055, -1.8013, -0.5811,  0.6816,  0.5072,
         -0.5991, -0.8345, -0.2359,  0.7858,  2.2680, -0.6472,  0.4782,  0.5260,
         -0.1092, -0.4317, -0.1673,  1.3168, -0.8270, -0.1828,  1.2467, -0.3471,
         -2.3646,  0.4888, -1.0780, -0.4192, -1.2580, -0.7447, -0.1837,  0.3522,
         -0.5779, -1.7977,  1.4424,  0.5041, -1.8134, -0.7771,  2.5258,  1.1374,
          0.4072, -0.6315, -0.4436, -0.9271, -0.1605, -0.4880,  0.0844, -0.4689,
         -1.7534,  0.0605, -0.1019,  0.2603,  1.2183,  1.1428, -0.2314, -0.0978]])
inputs: tensor([[-0.1658, -2.1646, -0.0329, -1.5946, -0.3023, -0.6994,  1.5663,  1.6034,
          1.8260,  0.8168,  0.2116, -0.1078, -0.1861,  0.0619,  0.4507, -2.9286,
         -0.1884,  2.0221, -1.0663,  0.5653, -0.4095,  0.8669, -1.0746,  0.1634,
          0.9368,  0.3213,  1.1310, -0.2696, -0.6441,  0.2774, -1.1574, -0.4615,
         -0.6046, -1.1028, -0.3971,  1.3249,  0.4932, -0.8154,  0.1505,  0.9680,
          0.4968, -0.0049,  0.4789, -0.2511, -1.0990, -0.3867, -0.9960, -0.8912,
          1.2027, -1.8490,  1.0418, -1.2771, -0.5703,  0.2846,  0.8555, -0.3869,
          1.0715, -2.4973, -0.8575,  0.4295,  0.2623,  1.5157, -1.0068,  1.8673],
        [ 0.1676, -1.3076,  0.5102,  0.5171,  0.0999,  1.4992, -0.5307,  0.4502,
         -0.2964, -0.8371,  0.3490,  0.5715, -1.6208,  0.2467,  1.1286, -1.6917,
          0.9864,  0.9376, -1.1601, -0.6994, -0.2375, -1.5894,  0.9233, -1.6602,
          0.2619,  1.0330,  0.4524, -0.0991, -1.3572, -0.2595, -0.5072,  0.3760,
          0.3425,  0.9836, -0.4024, -0.7606, -1.1793,  0.9026,  0.3326, -1.0521,
         -0.8507, -0.0251,  1.7143,  0.3544,  0.0443,  0.0406,  0.1709,  1.6408,
         -0.2123, -1.0160, -0.9764,  1.0370,  0.7496, -0.4754, -0.0363,  1.9517,
          0.9935, -0.0194,  2.2162, -0.2014,  0.8857,  0.4685,  1.3789, -0.4464]])
candidate_state: tensor([[ 0.3709, -0.1493, -0.0440, -0.0493,  0.0966,  0.2727,  0.8111, -0.0291,
         -0.0585, -0.1373, -0.0287,  1.1781,  0.3684, -0.1333,  0.7725,  0.5899,
         -0.0401,  0.3726,  0.3722, -0.1285, -0.1691, -0.1231, -0.1699,  0.3462,
         -0.0063,  0.2713, -0.0822,  0.5319, -0.0263,  0.4955, -0.1565, -0.1693,
          1.1113, -0.1479, -0.1552, -0.1607, -0.1106, -0.0240,  0.3590, -0.1685,
          0.0417,  0.5446,  0.2964, -0.1699,  0.0690, -0.0148, -0.1034, -0.0171,
         -0.1472,  0.4075, -0.1293, -0.1557,  0.9140, -0.1660,  0.6402,  0.5994,
          0.1156,  0.9624,  0.9501, -0.1081, -0.1530, -0.0925,  0.0457,  0.3305],
        [-0.0129, -0.1369,  0.2587, -0.0988,  0.0951,  0.1876, -0.1579,  0.1684,
          0.0983,  0.3567, -0.0448,  0.8229,  0.7252,  0.3303, -0.1203, -0.1466,
         -0.1222,  0.3475,  0.1340, -0.0694, -0.1670,  0.0311,  0.6597, -0.0898,
          0.6488, -0.1388,  0.1360, -0.0202,  0.3858, -0.0697, -0.1388, -0.0656,
         -0.1683,  0.0225, -0.1649,  0.6438,  0.5297, -0.1530,  0.0368,  0.4691,
         -0.1693, -0.0529,  0.0124, -0.1351,  0.6851, -0.0098,  0.2458,  1.1213,
          0.1516, -0.1695, -0.1639, -0.1700, -0.1688, -0.1694,  0.0197,  0.0492,
         -0.0962,  1.1609,  1.0537, -0.1437,  0.8302,  0.4547, -0.0519, -0.1523]])
new_state: tensor([[ 2.9445e-01,  4.5837e-01, -8.0933e-01,  9.4372e-01,  2.1188e-01,
          2.8049e-01, -8.9464e-02, -1.1808e+00,  5.2506e-01,  3.4022e-01,
          7.7829e-01,  7.0888e-01,  5.3170e-01,  6.6132e-01, -3.2209e-01,
          5.2269e-01,  1.4986e-01,  6.6759e-02,  5.2222e-03, -2.9142e-01,
         -3.5944e-01, -6.0886e-01,  1.0430e+00,  5.3122e-01,  7.3252e-01,
          3.3022e-01,  1.8070e-01, -2.3056e-01, -3.8230e-01, -4.9393e-01,
         -6.5579e-01,  5.6429e-01,  7.6119e-01, -7.1009e-01, -5.1436e-02,
         -2.6653e-01,  7.1486e-01, -2.3737e-01,  4.9194e-01,  3.4080e-01,
          2.6263e-01,  7.1234e-01,  1.2267e+00, -2.9668e-01,  5.8268e-01,
          3.1969e-01,  4.1439e-01,  2.8218e-01,  5.3403e-01, -3.9377e-01,
         -2.0400e-01, -1.5439e-01,  7.2541e-01, -1.0243e-03,  4.5229e-01,
          1.0553e+00, -9.8590e-01,  4.4133e-01,  3.3280e-01,  2.9011e-01,
          1.8359e-01,  3.1855e-01,  8.5932e-02,  2.6494e-02],
        [ 3.2560e-01, -4.1358e-01, -1.2777e-01, -1.7650e-01, -5.9459e-02,
         -4.5342e-01, -1.0455e-01, -1.6799e-01,  8.9184e-01,  1.5005e-01,
          2.2939e-01,  8.1460e-01, -4.8114e-01, -1.0486e-01,  2.6259e-01,
          1.6557e-01, -3.4991e-01, -2.1687e-01, -4.2631e-02,  3.3895e-01,
          9.9563e-01, -2.9275e-01,  5.7304e-01,  2.0422e-01,  2.8688e-01,
         -2.7864e-01, -8.8153e-03,  6.1819e-01, -1.9329e-01, -1.2374e-01,
          5.2273e-01, -2.0002e-01, -1.2170e+00,  2.4510e-01, -6.0092e-01,
          1.3625e-01, -3.2389e-01, -4.3553e-01, -6.8523e-02,  4.1328e-01,
         -3.6436e-01, -8.8597e-01,  6.9516e-01,  1.7006e-01, -5.0785e-01,
         -3.7618e-01,  1.3344e+00,  1.1290e+00,  2.7363e-01, -3.9006e-01,
         -2.9743e-01, -5.3149e-01, -1.6485e-01, -3.2149e-01,  5.0585e-02,
         -1.9817e-01, -8.8747e-01,  6.3553e-01,  5.0198e-01,  4.9243e-02,
          1.0155e+00,  7.8325e-01, -1.3759e-01, -1.2625e-01]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x76bbf29fb760>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
>       assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
E       assert tensor(0.5017) > tensor(0.5346)
E        +  where tensor(0.5017) = <built-in method mean of type object at 0x76bc2b8678c0>(tensor([[0.4878],\n        [0.5157]]))
E        +    where <built-in method mean of type object at 0x76bc2b8678c0> = torch.mean
E        +  and   tensor(0.5346) = <built-in method mean of type object at 0x76bc2b8678c0>(tensor([[0.5434],\n        [0.5259]]))
E        +    where <built-in method mean of type object at 0x76bc2b8678c0> = torch.mean

tests/unit/integration/test_state_management.py:97: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4878],
        [0.5157]])
state: tensor([[ 0.9674, -2.0603,  0.5146,  0.2178, -0.7979, -2.8292,  0.0561, -0.5667,
          0.2275,  0.2680, -1.1466,  1.5589,  0.9304,  0.9942, -0.0674, -1.8019,
          0.6932, -0.2394,  0.5426,  2.4932, -0.2204, -1.4137, -1.5877,  0.3175,
          0.9129,  0.0839,  1.2601, -0.3425,  0.6746, -0.3904,  0.6460, -0.3600,
         -0.8885, -0.8071,  0.0366, -0.0521,  0.8325,  0.5513,  1.5298, -0.7032,
         -0.9906, -0.3994,  0.0376,  0.3101,  0.0135, -0.5801, -0.7242,  1.3855,
         -1.8305, -0.3346, -1.3140,  0.5430,  0.0990, -0.7398, -0.5230, -0.8006,
          0.1650, -0.1616, -0.7503,  1.1882,  0.0909,  0.6421,  1.0552, -0.5142],
        [ 0.2891,  0.2591, -0.0567,  0.8753, -0.0097,  0.4924,  0.3368,  1.9913,
         -0.3089, -0.3571, -0.1392, -0.1783,  1.0139,  1.2756, -0.6618,  0.8364,
          1.8462, -0.1223, -0.0968, -3.1269, -0.9538, -0.0639,  0.6379,  0.0154,
          0.5945, -1.0038, -0.2551, -0.3284, -1.3450,  0.1136, -1.5470, -0.6966,
          2.1322, -0.2483, -0.7334, -0.4384,  0.0658, -0.3100, -0.1301, -0.0926,
          2.1012,  1.4460,  1.4167, -0.4301, -0.5267, -1.4352,  0.1644,  0.5332,
         -0.6990,  0.2701,  1.2660,  0.0296, -0.9221, -0.3233,  0.2086,  0.0671,
         -0.7872,  1.0362,  0.6036, -0.0815,  0.4590,  1.9447,  1.1266, -2.1963]])
inputs: tensor([[ 0.9000, -2.1381,  0.3641,  0.3230, -0.6219, -2.7775, -0.0205, -0.6379,
          0.1255,  0.2184, -1.0017,  1.5492,  0.9859,  0.9491, -0.1698, -1.8547,
          0.7015, -0.3145,  0.4805,  2.3956, -0.2297, -1.5423, -1.7351,  0.1624,
          1.0891,  0.1004,  1.2868, -0.4380,  0.6024, -0.3619,  0.4739, -0.3306,
         -0.8440, -0.8333,  0.1604, -0.0129,  0.8477,  0.4895,  1.5535, -0.8088,
         -0.9383, -0.4823,  0.0478,  0.3315, -0.0540, -0.4231, -0.7310,  1.2551,
         -1.8168, -0.4910, -1.3692,  0.6454,  0.0610, -0.8441, -0.4416, -0.7375,
          0.1160, -0.0035, -0.8949,  1.3495,  0.1637,  0.6982,  0.9501, -0.5358],
        [ 0.3095,  0.2985, -0.0743,  0.9893, -0.0508,  0.4987,  0.3338,  1.7955,
         -0.2400, -0.4053, -0.0787, -0.2167,  0.8862,  1.2248, -0.7786,  1.0335,
          1.7009, -0.0752, -0.1462, -3.4267, -0.8581, -0.1858,  0.6457,  0.0265,
          0.6007, -0.8257, -0.2382, -0.3342, -1.2722,  0.2173, -1.4680, -0.5299,
          2.0666, -0.3451, -0.7400, -0.4979,  0.1281, -0.1585, -0.1971, -0.2099,
          2.0135,  1.5367,  1.3989, -0.3431, -0.4431, -1.3122,  0.1725,  0.4602,
         -0.4338,  0.3568,  1.2966,  0.0958, -0.8771, -0.3216,  0.1374,  0.0755,
         -0.9442,  0.9281,  0.5410, -0.0040,  0.3597,  1.9602,  1.0940, -2.0772]])
candidate_state: tensor([[-0.0745, -0.1694, -0.1355, -0.1345, -0.1245,  1.1078,  0.0789,  0.1109,
         -0.1300, -0.1700, -0.0545, -0.1017, -0.1698,  0.2310, -0.0673, -0.0651,
          0.0398,  0.1315, -0.1531,  0.2132, -0.0033, -0.1695, -0.1474,  0.0736,
         -0.1681,  0.1221, -0.0058, -0.1619, -0.0132,  0.1115, -0.1687,  0.0700,
          0.5722, -0.1457,  0.1270, -0.0608,  0.0231,  0.1467, -0.1525,  0.5676,
         -0.0043,  0.2694,  0.3276, -0.0152,  0.4093,  0.0905, -0.0541, -0.0711,
          0.2361, -0.1622,  0.2308, -0.0622,  0.0632,  0.3598,  0.2917,  0.0883,
         -0.1667, -0.1146,  0.2787,  0.4769,  0.1570,  0.9934,  0.8869,  0.7545],
        [ 0.0231,  0.5437, -0.1665,  0.5350,  0.9410, -0.1333,  0.5179,  0.1127,
          0.3027,  0.9994,  0.5233,  1.1717,  0.0669,  0.0336, -0.0812,  1.2346,
          0.0909, -0.0819, -0.0728, -0.0686,  0.4637, -0.0242,  0.0868,  0.4609,
         -0.0895, -0.1688, -0.1562,  0.0590,  0.1126, -0.1693,  0.3466,  0.4854,
          0.3850,  0.2826,  0.2184,  0.5942,  0.4315,  0.0209,  0.4376, -0.1660,
         -0.1700, -0.1620, -0.0200, -0.1534,  0.1624, -0.1080, -0.1698,  0.0206,
          0.0129, -0.1699,  0.1010,  0.0729,  0.3716, -0.1699, -0.0864, -0.0088,
         -0.1665,  0.8128, -0.1664,  0.1804, -0.0223,  0.6691, -0.1616, -0.0795]])
new_state: tensor([[ 0.4337, -1.0917,  0.1816,  0.0373, -0.4529, -0.8125,  0.0678, -0.2196,
          0.0444,  0.0437, -0.5872,  0.7083,  0.3668,  0.6033, -0.0673, -0.9123,
          0.3585, -0.0494,  0.1862,  1.3253, -0.1092, -0.7764, -0.8499,  0.1925,
          0.3592,  0.1035,  0.6117, -0.2500,  0.3223, -0.1333,  0.2287, -0.1397,
         -0.1403, -0.4683,  0.0829, -0.0565,  0.4179,  0.3441,  0.6681, -0.0523,
         -0.4854, -0.0568,  0.1862,  0.1435,  0.2163, -0.2366, -0.3810,  0.6394,
         -0.7719, -0.2463, -0.5227,  0.2330,  0.0807, -0.1766, -0.1056, -0.3453,
         -0.0049, -0.1375, -0.2232,  0.8238,  0.1248,  0.8220,  0.9690,  0.1357],
        [ 0.1603,  0.3969, -0.1099,  0.7105,  0.4507,  0.1894,  0.4245,  1.0815,
         -0.0127,  0.2999,  0.1816,  0.4755,  0.5553,  0.6741, -0.3806,  1.0292,
          0.9961, -0.1027, -0.0852, -1.6458, -0.2673, -0.0447,  0.3710,  0.2312,
          0.2633, -0.5994, -0.2072, -0.1408, -0.6391, -0.0234, -0.6299, -0.1242,
          1.2860,  0.0088, -0.2724,  0.0617,  0.2429, -0.1498,  0.1448, -0.1281,
          1.0013,  0.6673,  0.7209, -0.2961, -0.1930, -0.7924,  0.0026,  0.2850,
         -0.3543,  0.0570,  0.7018,  0.0506, -0.2955, -0.2490,  0.0657,  0.0304,
         -0.4866,  0.9280,  0.2307,  0.0454,  0.2259,  1.3269,  0.5027, -1.1711]])
memory_gate: tensor([[0.5434],
        [0.5259]])
state: tensor([[ 0.9674, -2.0603,  0.5146,  0.2178, -0.7979, -2.8292,  0.0561, -0.5667,
          0.2275,  0.2680, -1.1466,  1.5589,  0.9304,  0.9942, -0.0674, -1.8019,
          0.6932, -0.2394,  0.5426,  2.4932, -0.2204, -1.4137, -1.5877,  0.3175,
          0.9129,  0.0839,  1.2601, -0.3425,  0.6746, -0.3904,  0.6460, -0.3600,
         -0.8885, -0.8071,  0.0366, -0.0521,  0.8325,  0.5513,  1.5298, -0.7032,
         -0.9906, -0.3994,  0.0376,  0.3101,  0.0135, -0.5801, -0.7242,  1.3855,
         -1.8305, -0.3346, -1.3140,  0.5430,  0.0990, -0.7398, -0.5230, -0.8006,
          0.1650, -0.1616, -0.7503,  1.1882,  0.0909,  0.6421,  1.0552, -0.5142],
        [ 0.2891,  0.2591, -0.0567,  0.8753, -0.0097,  0.4924,  0.3368,  1.9913,
         -0.3089, -0.3571, -0.1392, -0.1783,  1.0139,  1.2756, -0.6618,  0.8364,
          1.8462, -0.1223, -0.0968, -3.1269, -0.9538, -0.0639,  0.6379,  0.0154,
          0.5945, -1.0038, -0.2551, -0.3284, -1.3450,  0.1136, -1.5470, -0.6966,
          2.1322, -0.2483, -0.7334, -0.4384,  0.0658, -0.3100, -0.1301, -0.0926,
          2.1012,  1.4460,  1.4167, -0.4301, -0.5267, -1.4352,  0.1644,  0.5332,
         -0.6990,  0.2701,  1.2660,  0.0296, -0.9221, -0.3233,  0.2086,  0.0671,
         -0.7872,  1.0362,  0.6036, -0.0815,  0.4590,  1.9447,  1.1266, -2.1963]])
inputs: tensor([[ 1.9494, -0.9442,  1.5120, -2.2322, -0.0489, -0.2923, -0.1254,  0.5292,
         -0.7998, -0.2151, -0.0171,  1.7991, -0.8052,  1.1314, -0.3323,  0.6682,
          0.9533,  1.5306,  0.6873,  0.3420, -1.1934,  1.1260, -1.7118, -0.6617,
         -1.8594, -1.2355, -1.1308, -0.0680, -0.8821,  0.0739, -1.0808, -1.4375,
         -0.0617,  1.5895,  0.2555,  1.0380, -1.0541, -0.5079,  0.1454,  0.7009,
         -0.9460,  0.0877, -0.3496, -0.7039,  0.3245, -2.3485,  1.1428,  1.0593,
         -1.1694,  1.5117,  1.0475, -1.1931, -0.1331,  0.1287, -0.3637, -0.0571,
         -1.1848, -0.7607,  1.2026,  0.0122,  1.2164, -0.2405,  0.1832, -2.0093],
        [-0.2226,  0.5457, -0.5068,  0.9838,  0.5402,  0.9365,  0.1010,  1.0765,
         -0.7339, -0.9650, -0.2345, -0.3826,  1.4630, -1.2472, -0.8941, -0.2105,
          0.0370, -1.1312, -1.1456, -0.4985, -0.6203, -0.2792,  0.0363,  0.2011,
          0.6916, -1.4344, -0.7985,  0.1653,  0.1971, -1.3833,  0.0709, -0.6914,
         -1.6395, -0.7852, -1.1511,  1.1021, -0.5371, -1.8986, -0.8227, -1.3745,
         -0.6759, -1.8703, -0.0505,  0.1752, -2.5801,  0.1325,  1.7421, -1.5451,
          0.5990, -0.9552, -0.5769, -0.6351,  0.7891, -0.1997,  0.3663, -0.5461,
         -0.5022,  0.2555,  0.7247, -0.2968, -0.3633,  0.6106,  1.1289,  0.7847]])
candidate_state: tensor([[-0.1515,  0.0620,  0.0806,  0.5032, -0.1272,  0.1001,  0.3947, -0.1186,
         -0.1655,  0.0342, -0.0126, -0.0920, -0.1617, -0.1577, -0.0424,  0.1458,
         -0.0843, -0.1686,  0.6726,  0.0154,  0.4025, -0.1487,  0.0672, -0.1319,
         -0.1688,  1.2737,  0.2784, -0.0416, -0.0050,  0.0671,  0.7744,  0.1890,
         -0.1483,  0.1218,  0.7060,  0.6764,  0.0151, -0.0951, -0.1699,  0.4843,
          0.3573,  0.2276, -0.1699,  0.0033,  0.0909,  0.0071, -0.0113, -0.1346,
         -0.1699,  0.3354, -0.0865,  0.8120, -0.1109, -0.0191, -0.1040,  0.3690,
          0.1218, -0.1698, -0.1627,  0.0201, -0.1569, -0.1335, -0.1665, -0.1653],
        [-0.1517,  0.0201, -0.1278,  0.8213,  0.3850,  0.0690,  0.6175,  0.1938,
          0.1888,  0.0993,  0.4694, -0.0270,  1.0099, -0.0565, -0.1697, -0.0854,
         -0.0883, -0.0409, -0.1675, -0.0333, -0.0022, -0.0430,  0.4171,  0.0697,
         -0.0104, -0.1104,  0.3558,  0.4936,  0.3509, -0.1076,  0.1907, -0.0270,
          0.0597,  0.6601,  0.3668,  0.6008,  0.3652, -0.0483, -0.0780, -0.0738,
         -0.0449,  0.0252,  0.2183,  0.2306, -0.1161, -0.0364, -0.0540, -0.1579,
          0.3123,  0.3068,  1.4518, -0.0960,  0.2601, -0.1641,  0.3021, -0.1165,
         -0.1695,  0.4584,  1.2477, -0.1647,  0.0384,  0.5679, -0.1697,  0.5097]])
new_state: tensor([[ 0.4565, -1.0912,  0.3165,  0.3481, -0.4916, -1.4917,  0.2107, -0.3621,
          0.0481,  0.1613, -0.6288,  0.8051,  0.4318,  0.4682, -0.0560, -0.9126,
          0.3382, -0.2071,  0.6020,  1.3619,  0.0640, -0.8361, -0.8321,  0.1123,
          0.4190,  0.6272,  0.8119, -0.2051,  0.3643, -0.1815,  0.7046, -0.1093,
         -0.5505, -0.3830,  0.3422,  0.2805,  0.4593,  0.2562,  0.7537, -0.1610,
         -0.3752, -0.1131, -0.0571,  0.1700,  0.0488, -0.3120, -0.3987,  0.6914,
         -1.0723, -0.0287, -0.7535,  0.6658,  0.0032, -0.4107, -0.3316, -0.2666,
          0.1453, -0.1654, -0.4820,  0.6548, -0.0222,  0.2880,  0.4974, -0.3549],
        [ 0.0801,  0.1458, -0.0904,  0.8497,  0.1775,  0.2916,  0.4699,  1.1390,
         -0.0729, -0.1407,  0.1493, -0.1065,  1.0120,  0.6440, -0.4285,  0.3993,
          0.9290, -0.0837, -0.1303, -1.6601, -0.5026, -0.0540,  0.5332,  0.0412,
          0.3077, -0.5802,  0.0346,  0.0614, -0.5409,  0.0087, -0.7231, -0.3791,
          1.1495,  0.1824, -0.2117,  0.0543,  0.2078, -0.1859, -0.1054, -0.0837,
          1.0837,  0.7724,  0.8485, -0.1168, -0.3320, -0.7720,  0.0608,  0.2055,
         -0.2195,  0.2875,  1.3541, -0.0300, -0.3616, -0.2478,  0.2529, -0.0200,
         -0.4944,  0.7622,  0.9090, -0.1210,  0.2596,  1.2919,  0.5120, -0.9133]])
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x76bbf29f9240>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_phi_metric_computation(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Create sample inputs
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Initialize parameters
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-0.9166,  0.7973,  0.1561, -0.5011,  0.2046, -0.6714, -1.4456,
          -0.0028,  0.3446, -1.8615,  0.9154,...         0.5598, -0.0477,  0.5058, -1.3078,  1.5833,  1.7847, -1.2246,
           1.2777, -0.3941,  1.8829, -0.5686]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x76bbf29f8550>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_information_flow(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.zeros(batch_size, num_modules, input_dim, device=device)  # ensure shape matches the model
    
        # Test with and without dropout
        integration_module.train()
>       output1, _ = integration_module(inputs)

tests/unit/memory/test_integration.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestInformationIntegration.test_entropy_calculations _____________

self = <test_integration.TestInformationIntegration object at 0x76bbf29f8100>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_entropy_calculations(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = torch.ones(batch_size, num_modules, input_dim, device=device)
        integration_module.eval()
        with torch.no_grad():
>           _, phi_uniform = integration_module(uniform_input)

tests/unit/memory/test_integration.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1....., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x76bbf29f88e0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_integration(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Process through integration
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-0.3581,  0.5044,  0.5313, -0.5679, -0.3935,  0.9990,  1.4043,
          -1.3021,  0.3484,  0.2811, -1.7525,...         0.0797, -1.9822,  0.1911,  0.2929, -0.6524,  1.0103, -0.2995,
          -0.0333,  1.7950, -1.7242,  2.4691]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestStateManagement.test_state_value_estimation ________________

self = <test_consciousness_state_management.TestStateManagement object at 0x76bbf2974160>
state_manager = ConsciousnessStateManager(), batch_size = 2, hidden_dim = 64

    def test_state_value_estimation(self, state_manager, batch_size, hidden_dim):
        """Test state value estimation."""
        consciousness_state = torch.randn(batch_size, hidden_dim)
        integrated_output = torch.randn(batch_size, hidden_dim)
    
        # Test value estimation consistency
        _, metrics1 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
        _, metrics2 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
    
        # Same input should give same value estimate
>       assert torch.allclose(metrics1['state_value'], metrics2['state_value'], rtol=1e-5)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x76bc2b8678c0>(tensor([[-0.2432],\n        [ 0.2013]], grad_fn=<AddmmBackward0>), tensor([[-0.3094],\n        [ 0.0857]], grad_fn=<AddmmBackward0>), rtol=1e-05)
E        +    where <built-in method allclose of type object at 0x76bc2b8678c0> = torch.allclose

tests/unit/state/test_consciousness_state_management.py:100: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5477],
        [0.5063]], grad_fn=<SigmoidBackward0>)
state: tensor([[-3.2033,  0.9451,  0.6176, -1.0778,  0.4453,  0.0382,  1.0514, -0.5823,
         -0.8438,  1.3169, -2.2081, -0.2237,  1.8961,  0.4951,  1.2869, -2.0446,
         -0.1916, -0.0414,  1.4282,  0.4972, -1.5242, -0.6048,  2.0579,  0.2877,
         -0.9438,  0.0978,  0.4223,  1.4360,  0.2585,  1.8365,  0.0318, -0.5887,
         -0.6362, -0.0108, -0.1287,  1.3918, -0.8746,  0.4710,  0.3252,  0.3232,
          1.8862, -0.4896, -0.4590, -2.5134,  1.3731,  0.0351, -1.2820,  0.7143,
         -0.6240, -1.7243,  2.3602,  1.0842, -0.9839,  0.5775,  0.6773, -0.3854,
          0.5409, -0.9940, -0.6435,  0.4098, -0.9954,  0.3572,  0.3886,  0.1357],
        [ 0.2676,  1.0140,  0.3033, -0.6699,  0.5125,  0.2457, -3.1067,  0.9689,
         -2.2414,  0.0871, -2.2820,  1.7500,  1.1805,  0.2678,  0.2704, -0.3314,
          1.1489, -0.2333,  0.3668, -0.2527,  0.3828,  0.4199, -1.2819, -0.2474,
          0.4325, -0.6633,  1.1664,  0.4049,  0.7312, -1.7220, -0.6196, -0.0523,
         -0.3995,  0.3675,  0.8121, -0.4898,  1.3782,  0.6393,  1.6146,  0.8381,
         -0.6467,  0.5260, -2.0494,  0.6586, -2.5849, -0.3419,  0.4101, -0.1787,
          0.6618, -0.2560,  0.9718,  0.5628, -1.2115,  0.2860, -1.5475, -0.5121,
          0.3093,  1.0423,  0.4278, -0.5127, -0.7956, -1.2667, -1.7513, -1.1557]])
inputs: tensor([[ 1.7113,  1.5290, -0.2914, -1.7717, -0.2269,  0.3206, -0.5684,  0.4080,
          0.3448,  1.2382, -0.4823, -0.8351,  0.2478, -0.8769,  0.0167, -0.7902,
         -0.0864,  0.2735,  0.5820,  0.0226,  0.5893,  0.4815,  0.9806,  1.0880,
         -0.1444, -1.4867, -1.2628,  1.0680,  0.7788,  0.2330, -0.5548, -0.5892,
          1.0308, -1.7110,  1.1118,  1.5626, -0.1315,  1.5526,  1.4490, -0.4404,
         -0.0299,  1.0081,  1.1631, -0.7873, -1.6617,  0.4556, -0.8940, -0.8881,
          0.0560, -0.3945,  0.2751,  0.9065,  1.3648, -0.5344,  1.5317, -0.9002,
         -0.5955, -1.0623, -0.0143,  0.3123,  0.0447, -0.2981, -1.9312,  0.1010],
        [-1.4331,  0.5891, -0.8916,  0.5402, -0.1154,  0.4728,  0.5718,  0.3241,
          0.7885,  1.4792, -1.8175,  0.6475,  1.7073,  0.0523,  1.0277, -0.6475,
          0.8287,  0.3785, -0.3504,  1.3070,  0.6498,  0.8703, -1.5599,  2.0172,
          0.2844,  0.7561, -1.0533,  0.7592, -1.1274,  1.1853, -0.4550,  1.1720,
         -0.0540, -0.3960,  1.1657,  1.7324,  2.0142, -0.7070,  1.0780,  1.2416,
         -0.1257, -0.2284,  0.6025,  0.6456,  0.3064, -2.3229, -1.3165, -1.4978,
          0.4376, -0.7646,  0.9954,  0.1049, -1.1273,  0.2633,  0.0767, -1.0635,
         -0.4342, -0.1217,  2.7405, -0.4657, -0.0460, -0.7284, -0.0262,  0.3308]])
candidate_state: tensor([[-0.1676,  0.3192, -0.0751,  0.0039, -0.0412, -0.0601, -0.0331,  0.2883,
          0.0645, -0.0088,  0.1604, -0.1599,  0.4397,  0.0360, -0.1301, -0.1464,
         -0.1156,  0.1119,  0.0891,  1.2185,  0.2948, -0.0862, -0.1616, -0.1467,
         -0.1112,  0.0716, -0.0827,  0.0914,  0.2929,  0.3979, -0.1326,  0.4843,
         -0.0096,  0.5308, -0.1268,  0.0546,  0.1450, -0.0401,  0.0789,  0.0148,
         -0.1294, -0.0939,  0.5861, -0.1700, -0.1182, -0.0241,  0.1766,  0.1390,
         -0.0711, -0.1631, -0.1657, -0.0074, -0.0761,  0.0997, -0.0336, -0.1654,
          0.5601, -0.1582, -0.1676,  0.5537,  0.1203, -0.1699, -0.1374, -0.0882],
        [-0.1242,  0.1071, -0.1540,  0.1475,  0.6061,  0.5504,  0.2624, -0.1463,
          0.2237, -0.1275,  0.6054, -0.1635,  0.3788, -0.1108,  0.0797,  0.6201,
         -0.1612,  0.6585,  0.2488,  0.3266,  1.0638,  0.2963,  0.0952,  0.1974,
         -0.1227, -0.0955,  0.1807,  0.0088,  0.5983,  0.4660,  0.4041, -0.1673,
          0.1687,  0.1253, -0.1425,  1.8837,  0.1594,  1.0128, -0.0931,  0.0378,
         -0.0180,  0.4138,  0.0315, -0.0950,  0.8277, -0.1551, -0.0748,  0.0144,
         -0.0639, -0.1691,  0.4383,  1.0445, -0.1700,  0.0955,  0.3638,  0.0416,
          0.2858,  0.1354, -0.1581, -0.0687, -0.1670, -0.1261, -0.1370,  0.3722]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[-1.8302,  0.6620,  0.3043, -0.5886,  0.2252, -0.0063,  0.5608, -0.1885,
         -0.4330,  0.7172, -1.1367, -0.1948,  1.2373,  0.2874,  0.6459, -1.1860,
         -0.1572,  0.0279,  0.8225,  0.8235, -0.7014, -0.3702,  1.0540,  0.0912,
         -0.5672,  0.0860,  0.1938,  0.8278,  0.2741,  1.1858, -0.0426, -0.1033,
         -0.3528,  0.2342, -0.1279,  0.7869, -0.4134,  0.2398,  0.2138,  0.1837,
          0.9745, -0.3106,  0.0138, -1.4534,  0.6985,  0.0083, -0.6222,  0.4541,
         -0.3739, -1.0181,  1.2176,  0.5905, -0.5733,  0.3614,  0.3557, -0.2859,
          0.5496, -0.6160, -0.4282,  0.4749, -0.4907,  0.1187,  0.1507,  0.0344],
        [ 0.0742,  0.5662,  0.0775, -0.2664,  0.5587,  0.3961, -1.4433,  0.4183,
         -1.0243, -0.0189, -0.8564,  0.8053,  0.7847,  0.0809,  0.1763,  0.1384,
          0.5021,  0.2070,  0.3086,  0.0333,  0.7190,  0.3589, -0.6020, -0.0278,
          0.1584, -0.3830,  0.6798,  0.2094,  0.6656, -0.6418, -0.1142, -0.1091,
         -0.1190,  0.2479,  0.3408,  0.6821,  0.7764,  0.8237,  0.7715,  0.4430,
         -0.3363,  0.4706, -1.0221,  0.2865, -0.9001, -0.2497,  0.1707, -0.0834,
          0.3035, -0.2131,  0.7084,  0.8006, -0.6973,  0.1920, -0.6038, -0.2387,
          0.2977,  0.5946,  0.1385, -0.2935, -0.4852, -0.7036, -0.9543, -0.4013]],
       grad_fn=<AddBackward0>)
memory_gate: tensor([[0.5439],
        [0.4724]], grad_fn=<SigmoidBackward0>)
state: tensor([[-3.2033,  0.9451,  0.6176, -1.0778,  0.4453,  0.0382,  1.0514, -0.5823,
         -0.8438,  1.3169, -2.2081, -0.2237,  1.8961,  0.4951,  1.2869, -2.0446,
         -0.1916, -0.0414,  1.4282,  0.4972, -1.5242, -0.6048,  2.0579,  0.2877,
         -0.9438,  0.0978,  0.4223,  1.4360,  0.2585,  1.8365,  0.0318, -0.5887,
         -0.6362, -0.0108, -0.1287,  1.3918, -0.8746,  0.4710,  0.3252,  0.3232,
          1.8862, -0.4896, -0.4590, -2.5134,  1.3731,  0.0351, -1.2820,  0.7143,
         -0.6240, -1.7243,  2.3602,  1.0842, -0.9839,  0.5775,  0.6773, -0.3854,
          0.5409, -0.9940, -0.6435,  0.4098, -0.9954,  0.3572,  0.3886,  0.1357],
        [ 0.2676,  1.0140,  0.3033, -0.6699,  0.5125,  0.2457, -3.1067,  0.9689,
         -2.2414,  0.0871, -2.2820,  1.7500,  1.1805,  0.2678,  0.2704, -0.3314,
          1.1489, -0.2333,  0.3668, -0.2527,  0.3828,  0.4199, -1.2819, -0.2474,
          0.4325, -0.6633,  1.1664,  0.4049,  0.7312, -1.7220, -0.6196, -0.0523,
         -0.3995,  0.3675,  0.8121, -0.4898,  1.3782,  0.6393,  1.6146,  0.8381,
         -0.6467,  0.5260, -2.0494,  0.6586, -2.5849, -0.3419,  0.4101, -0.1787,
          0.6618, -0.2560,  0.9718,  0.5628, -1.2115,  0.2860, -1.5475, -0.5121,
          0.3093,  1.0423,  0.4278, -0.5127, -0.7956, -1.2667, -1.7513, -1.1557]])
inputs: tensor([[ 1.7113,  1.5290, -0.2914, -1.7717, -0.2269,  0.3206, -0.5684,  0.4080,
          0.3448,  1.2382, -0.4823, -0.8351,  0.2478, -0.8769,  0.0167, -0.7902,
         -0.0864,  0.2735,  0.5820,  0.0226,  0.5893,  0.4815,  0.9806,  1.0880,
         -0.1444, -1.4867, -1.2628,  1.0680,  0.7788,  0.2330, -0.5548, -0.5892,
          1.0308, -1.7110,  1.1118,  1.5626, -0.1315,  1.5526,  1.4490, -0.4404,
         -0.0299,  1.0081,  1.1631, -0.7873, -1.6617,  0.4556, -0.8940, -0.8881,
          0.0560, -0.3945,  0.2751,  0.9065,  1.3648, -0.5344,  1.5317, -0.9002,
         -0.5955, -1.0623, -0.0143,  0.3123,  0.0447, -0.2981, -1.9312,  0.1010],
        [-1.4331,  0.5891, -0.8916,  0.5402, -0.1154,  0.4728,  0.5718,  0.3241,
          0.7885,  1.4792, -1.8175,  0.6475,  1.7073,  0.0523,  1.0277, -0.6475,
          0.8287,  0.3785, -0.3504,  1.3070,  0.6498,  0.8703, -1.5599,  2.0172,
          0.2844,  0.7561, -1.0533,  0.7592, -1.1274,  1.1853, -0.4550,  1.1720,
         -0.0540, -0.3960,  1.1657,  1.7324,  2.0142, -0.7070,  1.0780,  1.2416,
         -0.1257, -0.2284,  0.6025,  0.6456,  0.3064, -2.3229, -1.3165, -1.4978,
          0.4376, -0.7646,  0.9954,  0.1049, -1.1273,  0.2633,  0.0767, -1.0635,
         -0.4342, -0.1217,  2.7405, -0.4657, -0.0460, -0.7284, -0.0262,  0.3308]])
candidate_state: tensor([[-3.5657e-02, -1.3554e-01,  7.8481e-02,  2.2749e-01,  3.1466e-01,
          2.3237e-01, -6.2307e-02, -1.3500e-01, -1.6715e-01, -1.6193e-01,
          2.4071e-01,  2.0898e-01,  5.7460e-01, -1.6911e-01, -1.1960e-01,
         -4.0125e-02,  2.3491e-01, -1.6981e-01,  4.3349e-01,  3.4245e-01,
          5.4846e-01,  1.8642e-01, -1.2499e-01,  3.7403e-01,  3.6570e-02,
         -1.2923e-01, -6.9828e-02,  3.3255e-02, -9.1769e-02,  1.8048e-01,
          2.7937e-01,  1.0498e-01,  4.1111e-03, -3.2908e-02,  5.7969e-01,
         -1.6772e-01,  2.2429e-01, -1.6175e-01,  1.7080e-01,  4.4828e-01,
         -2.8868e-02, -8.9143e-02,  4.1424e-01, -1.6316e-01,  7.1264e-01,
         -7.9281e-02, -1.3054e-01, -1.6961e-01, -1.5595e-01,  5.4370e-01,
         -1.6779e-01, -1.6740e-01,  9.2290e-02, -1.5183e-01, -4.6091e-02,
          6.4207e-01,  8.4739e-01,  5.3799e-01, -1.1313e-01, -1.6575e-01,
         -1.4274e-01,  7.1354e-01,  5.7480e-01,  9.4499e-02],
        [ 1.9494e-01,  3.3113e-01, -1.6095e-01,  9.8643e-01,  3.9754e-02,
         -1.6847e-01, -1.6881e-01, -1.6988e-01,  1.1817e-01,  5.2182e-01,
         -9.1033e-02, -4.4900e-02, -1.6956e-01, -1.0695e-01, -9.9086e-02,
          2.2703e-01,  3.7635e-01, -1.5575e-01, -1.3532e-01, -5.1252e-02,
          3.1123e-01, -1.3847e-01, -7.0641e-02, -6.7751e-02, -1.5836e-01,
         -1.6924e-01, -4.3736e-04, -1.2280e-01,  1.6380e-01,  8.2187e-02,
          2.9025e-01,  6.7613e-02, -1.0968e-01,  4.2990e-01,  5.7446e-01,
          6.9179e-02,  1.4693e+00, -1.4443e-01, -9.5578e-02,  4.0361e-03,
         -1.5839e-01,  2.7662e-01, -2.3948e-03,  1.1461e-01,  5.2321e-01,
         -1.6631e-01, -1.9573e-02,  2.4668e-01,  2.9546e-01, -1.6779e-01,
          1.4718e-01, -1.4099e-01,  6.5858e-01, -1.5227e-01,  6.4066e-03,
          7.8383e-01, -1.5691e-01,  3.1182e-01,  3.5265e-01, -1.3352e-01,
          5.4030e-01,  3.6744e-01,  9.0981e-03,  1.0814e+00]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[-1.7586,  0.4522,  0.3717, -0.4825,  0.3857,  0.1268,  0.5435, -0.3783,
         -0.5352,  0.6424, -1.0912, -0.0264,  1.2934,  0.1921,  0.6454, -1.1304,
          0.0029, -0.1000,  0.9745,  0.4266, -0.5789, -0.2439,  1.0623,  0.3271,
         -0.4967, -0.0058,  0.1978,  0.7962,  0.0987,  1.0812,  0.1447, -0.2723,
         -0.3441, -0.0209,  0.1944,  0.6805, -0.3734,  0.1824,  0.2548,  0.3803,
          1.0128, -0.3069, -0.0607, -1.4414,  1.0719, -0.0171, -0.7568,  0.3112,
         -0.4105, -0.6899,  1.2072,  0.5134, -0.4931,  0.2449,  0.3473,  0.0832,
          0.6807, -0.2953, -0.4016,  0.1473, -0.6065,  0.5197,  0.4735,  0.1169],
        [ 0.2293,  0.6537,  0.0583,  0.2040,  0.2631,  0.0272, -1.5566,  0.3681,
         -0.9964,  0.3164, -1.1260,  0.8030,  0.4682,  0.0701,  0.0755, -0.0368,
          0.7413, -0.1924,  0.1019, -0.1464,  0.3450,  0.1253, -0.6428, -0.1526,
          0.1207, -0.4026,  0.5508,  0.1265,  0.4318, -0.7701, -0.1395,  0.0110,
         -0.2466,  0.4004,  0.6867, -0.1949,  1.4263,  0.2258,  0.7123,  0.3980,
         -0.3890,  0.3944, -0.9694,  0.3716, -0.9450, -0.2492,  0.1834,  0.0457,
          0.4685, -0.2095,  0.5367,  0.1915, -0.2248,  0.0548, -0.7276,  0.1716,
          0.0633,  0.6569,  0.3881, -0.3126, -0.0908, -0.4045, -0.8225,  0.0247]],
       grad_fn=<AddBackward0>)
=============================== warnings summary ===============================
../../.local/lib/python3.10/site-packages/torch/__init__.py:1144
  /home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
    _C._set_default_tensor_type(t)

tests/unit/integration/test_state_management.py: 4 warnings
tests/unit/state/test_consciousness_state_management.py: 6 warnings
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py: 4 warnings
tests/unit/state/test_consciousness_state_management.py: 6 warnings
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = torch.tensor(inputs, dtype=torch.float32)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_environment.py::EnvironmentTests::test_core_imports - Runti...
FAILED tests/test_environment.py::EnvironmentTests::test_framework_versions
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_scaled_dot_product
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_attention_mask
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_consciousness_broadcasting
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_global_workspace_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_value_estimation
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_config
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_gru_state_updates
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_sequence_processing
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_context_aware_gating
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_information_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_retention
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_state_updates
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_reset_gate
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_sequence_processing
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_memory_retention
======= 13 failed, 15 passed, 2 skipped, 21 warnings, 19 errors in 0.77s =======
