============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l3-experiment
plugins: cov-6.0.0, anyio-4.7.0
collected 49 items

tests/test_consciousness.py EEEEEE                                       [ 12%]
tests/test_environment.py FFs...s                                        [ 26%]
tests/unit/attention/test_attention.py FFFF.                             [ 36%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 44%]
tests/unit/integration/test_cognitive_integration.py EEEE                [ 53%]
tests/unit/integration/test_state_management.py F.F.                     [ 61%]
tests/unit/memory/test_integration.py FFFF                               [ 69%]
tests/unit/memory/test_memory.py EEEEE                                   [ 79%]
tests/unit/memory/test_memory_components.py EEEE                         [ 87%]
tests/unit/state/test_consciousness_state_management.py ...F..           [100%]

==================================== ERRORS ====================================
______ ERROR at setup of TestConsciousnessModel.test_model_initialization ______

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405a980>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_forward_pass _______

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405a7d0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
__________ ERROR at setup of TestConsciousnessModel.test_model_config __________

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405a620>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
___ ERROR at setup of TestConsciousnessModel.test_model_state_initialization ___

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405a410>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_state_update _______

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405aa70>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
____ ERROR at setup of TestConsciousnessModel.test_model_attention_weights _____

self = <test_consciousness.TestConsciousnessModel object at 0x7e13f405a1a0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim=64, num_heads=4):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...yQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
        self.working_memory = WorkingMemory(
            input_dim=hidden_dim,  # Add this line
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
    
        # Information integration component
        self.information_integration = InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=num_layers,
            dropout_rate=dropout_rate
        )
    
        # Cognitive process integration
>       self.cognitive_integration = CognitiveProcessIntegration(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate
        )
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'dropout_rate'

models/consciousness_model.py:48: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cross_modal_attention _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7e13f405d2d0>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_modality_specific_processing _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7e13f405d480>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_integration_stability _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7e13f405d420>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cognitive_integration _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7e13f405c4c0>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
________ ERROR at setup of TestMemoryComponents.test_gru_state_updates _________

self = <test_memory.TestMemoryComponents object at 0x7e142e3c38b0>
hidden_dim = 64

    @pytest.fixture
    def gru_cell(self, hidden_dim):
        """Create GRU cell for testing."""
>       return GRUCell(hidden_dim=hidden_dim).to(self.device)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:50: TypeError
____ ERROR at setup of TestMemoryComponents.test_memory_sequence_processing ____

self = <test_memory.TestMemoryComponents object at 0x7e13f4054040>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_______ ERROR at setup of TestMemoryComponents.test_context_aware_gating _______

self = <test_memory.TestMemoryComponents object at 0x7e13f4054220>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____ ERROR at setup of TestMemoryComponents.test_information_integration ______

self = <test_memory.TestMemoryComponents object at 0x7e13f4054400>
hidden_dim = 64

    @pytest.fixture
    def info_integration(self, hidden_dim):
        """Create information integration module for testing."""
        return InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=4,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7e13f4054400>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_________ ERROR at setup of TestMemoryComponents.test_memory_retention _________

self = <test_memory.TestMemoryComponents object at 0x7e13f40545e0>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____________ ERROR at setup of TestGRUCell.test_gru_state_updates _____________

self = <test_memory_components.TestGRUCell object at 0x7e13f4054f10>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
______________ ERROR at setup of TestGRUCell.test_gru_reset_gate _______________

self = <test_memory_components.TestGRUCell object at 0x7e13f40550c0>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
_________ ERROR at setup of TestWorkingMemory.test_sequence_processing _________

self = <test_memory_components.TestWorkingMemory object at 0x7e13f40557e0>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
__________ ERROR at setup of TestWorkingMemory.test_memory_retention ___________

self = <test_memory_components.TestWorkingMemory object at 0x7e13f4055990>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
=================================== FAILURES ===================================
______________________ EnvironmentTests.test_core_imports ______________________

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
>           import torchvision

tests/test_environment.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164: in <module>
    def meta_nms(dets, scores, iou_threshold):
../../.local/lib/python3.10/site-packages/torch/library.py:795: in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
../../.local/lib/python3.10/site-packages/torch/library.py:184: in _register_fake
    handle = entry.fake_impl.register(func_to_register, source)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch._library.fake_impl.FakeImplHolder object at 0x7e13f3f4f3a0>
func = <function meta_nms at 0x7e13f3fdc430>
source = '/home/kasinadhsarma/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164'

    def register(self, func: Callable, source: str) -> RegistrationHandle:
        """Register an fake impl.
    
        Returns a RegistrationHandle that one can use to de-register this
        fake impl.
        """
        if self.kernel is not None:
            raise RuntimeError(
                f"register_fake(...): the operator {self.qualname} "
                f"already has an fake impl registered at "
                f"{self.kernel.source}."
            )
>       if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
E       RuntimeError: operator torchvision::nms does not exist

../../.local/lib/python3.10/site-packages/torch/_library/fake_impl.py:31: RuntimeError
___________________ EnvironmentTests.test_framework_versions ___________________

self = <test_environment.EnvironmentTests testMethod=test_framework_versions>

    def test_framework_versions(self):
        """Verify framework versions"""
        import torch
>       import torchvision

tests/test_environment.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26: in <module>
    def meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function meta_roi_align at 0x7e13f3fde050>

    def wrapper(fn):
>       if torchvision.extension._has_ops():
E       AttributeError: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)

../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18: AttributeError
_______________ TestAttentionMechanisms.test_scaled_dot_product ________________

self = <test_attention.TestAttentionMechanisms object at 0x7e13f405f670>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_scaled_dot_product(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test scaled dot-product attention computation."""
        # Create inputs
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:46: TypeError
_________________ TestAttentionMechanisms.test_attention_mask __________________

self = <test_attention.TestAttentionMechanisms object at 0x7e13f405f880>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_attention_mask(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test attention mask handling."""
        # Create inputs and mask
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:61: TypeError
___________ TestAttentionMechanisms.test_consciousness_broadcasting ____________

self = <test_attention.TestAttentionMechanisms object at 0x7e13f405fa90>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_consciousness_broadcasting(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test consciousness-aware broadcasting."""
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:74: TypeError
__________ TestAttentionMechanisms.test_global_workspace_integration ___________

self = <test_attention.TestAttentionMechanisms object at 0x7e13f405fc70>
batch_size = 2, seq_length = 8, hidden_dim = 128, num_heads = 4

    def test_global_workspace_integration(self, batch_size, seq_length, hidden_dim, num_heads):
        """Test global workspace integration."""
        workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=0.1
        )
    
>       inputs = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:93: TypeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_state_management.TestConsciousnessStateManager object at 0x7e13f405dbd0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4935],
        [0.5038]])
state: tensor([[ 9.6785e-01, -3.2173e-01, -1.1311e+00,  5.1557e-01, -4.4832e-01,
         -1.2268e+00,  6.4161e-02,  5.4528e-01, -5.4751e-01,  6.6602e-01,
         -2.3038e-01,  3.3155e-01, -1.5899e+00,  2.2532e+00,  1.2273e+00,
          1.4729e+00,  1.2444e+00, -1.5114e+00,  5.2165e-03, -7.5441e-01,
         -7.6085e-01,  1.5229e+00, -4.7969e-01, -7.8796e-01,  1.2221e+00,
          1.5063e+00,  6.2118e-01,  2.0379e-01, -1.2937e+00, -1.0057e-01,
          1.2549e+00,  5.1888e-01, -1.8970e+00, -8.3666e-02,  2.6152e-01,
         -2.0399e+00,  7.7477e-01,  4.5472e-01,  3.6080e-01,  1.7790e-02,
          1.4728e-01, -8.1543e-01, -3.2999e-01, -9.3413e-01,  9.8041e-01,
          5.3319e-01, -1.8609e-01, -3.9218e-01, -6.7798e-01,  2.4594e-01,
          1.1692e+00,  1.9046e+00, -9.4053e-01, -6.9557e-01, -1.2936e-01,
         -5.2895e-03, -4.2094e-01,  4.1154e-01,  4.6799e-01, -9.4727e-01,
          6.8528e-01, -3.2495e-01, -4.6590e-01,  6.8358e-01],
        [ 8.6616e-02, -1.0578e+00,  1.0813e+00,  7.6357e-01, -7.8106e-01,
         -1.4715e+00,  4.0947e-01,  1.0120e+00,  6.1442e-01,  8.4226e-01,
         -7.6030e-01, -5.2284e-02,  1.4552e+00, -5.6001e-01,  1.7122e+00,
         -1.2669e+00,  7.8409e-04,  3.4590e-01, -3.9531e-01, -1.4117e+00,
          8.0085e-01, -1.2131e+00,  1.7187e-01,  7.0165e-01,  1.3930e+00,
         -1.1468e+00,  2.0549e+00,  1.5083e-01, -7.9164e-01, -6.7728e-01,
          8.9474e-01,  1.7746e+00,  1.3910e+00,  8.8497e-01, -9.5717e-01,
         -2.1513e-01, -1.9153e-01, -7.7630e-01, -2.7623e-01, -1.0191e+00,
         -1.0304e+00,  5.3914e-01, -5.2562e-01, -1.8506e+00,  7.5420e-01,
         -9.0964e-01,  1.0094e-02,  2.7930e-01, -3.5419e-01, -1.6310e+00,
         -3.2495e-01,  6.0849e-01, -1.3759e+00,  9.5002e-01,  2.8849e-01,
          4.1121e-03,  1.7791e-01,  9.4234e-01, -1.0704e+00,  8.6924e-01,
         -2.0326e+00,  5.7718e-01,  1.5752e+00, -8.0093e-01]])
inputs: tensor([[ 0.5502, -0.0317, -0.4407, -1.7962, -0.4036,  0.4113, -0.0909,  0.2429,
          1.6613, -0.4836, -0.6523,  0.7075,  1.1356, -1.2732,  0.7509, -0.6127,
         -0.8388,  1.9113, -0.2447,  0.7221,  0.9417,  0.5564,  0.4756,  1.6396,
          0.9344, -2.3365,  1.2813, -0.1858, -1.7224, -0.5452,  0.3845, -0.6463,
          0.0737,  0.5293, -1.0900,  0.7311,  0.0286, -0.2087, -0.1683, -0.4755,
         -0.9790,  1.2927,  0.9267,  1.1093,  0.6756,  0.5159, -0.7989,  0.8290,
          0.8151,  0.5981,  0.3169,  0.9888,  0.2184, -1.2032, -1.0454, -1.6522,
         -0.4544,  1.5869, -3.1493, -0.2937,  1.9480, -1.6514,  2.1005, -0.5489],
        [ 0.6034, -0.3485,  0.4401, -1.6754, -0.0502,  0.6976, -0.1117, -0.6364,
          0.7848, -0.5487, -1.3747, -1.5511,  0.9811, -1.9775,  0.6054, -1.1873,
         -1.2756,  1.4672, -0.1751,  0.2005,  0.2529,  0.8452,  0.7137,  0.0180,
         -1.0164, -0.4004,  0.4776, -0.6810, -0.7102,  1.6136,  0.4111,  1.1929,
         -0.7569,  0.6835, -0.2403,  1.1713, -0.2752, -0.3630,  0.3399,  0.9414,
          0.0597,  0.0564, -0.3746,  0.4963,  0.9847, -0.9542,  0.5059, -0.7347,
          0.7348, -1.0126,  0.7863,  0.3049,  0.4096, -0.3240,  0.6661,  0.4323,
          1.0542, -1.1954, -0.5964, -2.0695, -0.3406, -2.1333, -0.8437, -0.0533]])
candidate_state: tensor([[-0.1274,  0.0273,  0.0281, -0.1419,  0.4035,  0.0525, -0.0930, -0.1382,
          0.5885,  1.5205, -0.0305, -0.0991, -0.1293, -0.1680,  0.4584, -0.1503,
         -0.0509,  0.0270, -0.0359,  1.3457,  0.7742, -0.1132,  0.1319, -0.1512,
         -0.1468, -0.1641, -0.1245, -0.1093,  1.6513,  0.2293,  0.1609, -0.0026,
          0.0852,  0.0362,  0.6319,  0.2614, -0.1689,  0.4072,  0.6108, -0.0499,
          0.9629,  0.1944,  0.2764,  0.1850,  1.7174,  0.2101,  0.4097,  0.0259,
         -0.1258, -0.1009,  1.2038, -0.0932, -0.0982, -0.1243,  0.4790,  0.0204,
          0.5524,  0.9597,  0.0185,  0.1370, -0.1354, -0.1693, -0.0197, -0.0818],
        [ 0.0512,  0.1113, -0.1474, -0.0993,  0.2380,  0.3554,  0.1512,  0.2056,
          0.0963, -0.0877,  0.2058,  0.2773, -0.1236, -0.0887,  0.9418, -0.1671,
         -0.0412, -0.1529, -0.1621,  0.0429,  0.2391, -0.1663,  0.7846,  0.1389,
          0.0518, -0.0900, -0.1080, -0.0538,  0.8128,  1.3479, -0.1156, -0.0950,
          0.0414, -0.0858, -0.0274, -0.1607,  0.0588, -0.1422, -0.0777,  0.7418,
         -0.1607,  0.0464, -0.0453, -0.0518,  0.0875,  0.1337,  0.2007,  0.6534,
          0.0063, -0.1457,  0.2373, -0.1660,  0.1688,  0.6358,  0.1587, -0.0595,
          0.5835, -0.1546,  0.6236,  0.1816, -0.1691, -0.1577,  0.0274, -0.0860]])
new_state: tensor([[ 0.4131, -0.1449, -0.5440,  0.1826, -0.0169, -0.5788, -0.0154,  0.1991,
          0.0278,  1.0988, -0.1291,  0.1134, -0.8501,  1.0269,  0.8379,  0.6508,
          0.5883, -0.7322, -0.0156,  0.3093,  0.0166,  0.6942, -0.1699, -0.4655,
          0.5288,  0.6603,  0.2435,  0.0452,  0.1979,  0.0665,  0.7008,  0.2548,
         -0.8931, -0.0230,  0.4491, -0.8744,  0.2968,  0.4306,  0.4874, -0.0165,
          0.5604, -0.3040, -0.0229, -0.3673,  1.3537,  0.3695,  0.1157, -0.1804,
         -0.3983,  0.0703,  1.1867,  0.8927, -0.5139, -0.4062,  0.1787,  0.0077,
          0.0720,  0.6892,  0.2403, -0.3981,  0.2696, -0.2461, -0.2399,  0.2959],
        [ 0.0691, -0.4777,  0.4717,  0.3355, -0.2754, -0.5650,  0.2813,  0.6119,
          0.3574,  0.3809, -0.2810,  0.1112,  0.6718, -0.3261,  1.3300, -0.7212,
         -0.0201,  0.0984, -0.2796, -0.6900,  0.5221, -0.6937,  0.4759,  0.4224,
          0.7275, -0.6224,  0.9817,  0.0493,  0.0044,  0.3276,  0.3934,  0.8470,
          0.7213,  0.4033, -0.4959, -0.1881, -0.0673, -0.4617, -0.1777, -0.1454,
         -0.5988,  0.2946, -0.2873, -0.9581,  0.4234, -0.3920,  0.1047,  0.4649,
         -0.1753, -0.8940, -0.0460,  0.2242, -0.6095,  0.7941,  0.2241, -0.0274,
          0.3791,  0.3981, -0.2299,  0.5280, -1.1080,  0.2126,  0.8072, -0.4462]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x7e13f405df30>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
>       assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
E       assert tensor(0.5005) > tensor(0.5022)
E        +  where tensor(0.5005) = <built-in method mean of type object at 0x7e142d0678c0>(tensor([[0.5147],\n        [0.4862]]))
E        +    where <built-in method mean of type object at 0x7e142d0678c0> = torch.mean
E        +  and   tensor(0.5022) = <built-in method mean of type object at 0x7e142d0678c0>(tensor([[0.5009],\n        [0.5035]]))
E        +    where <built-in method mean of type object at 0x7e142d0678c0> = torch.mean

tests/unit/integration/test_state_management.py:97: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5147],
        [0.4862]])
state: tensor([[ 9.7886e-01,  2.6937e-01,  8.3048e-01,  2.2628e+00,  3.2685e-02,
          1.5868e-03, -1.3277e+00, -1.0255e+00, -8.2196e-02,  7.8156e-01,
          2.9762e-01,  7.1066e-01, -3.4675e-01,  1.5828e+00, -1.3853e+00,
          5.3543e-01, -5.0805e-01, -2.0711e+00,  1.7787e-01,  1.6572e+00,
          7.3316e-01, -1.5413e+00, -2.3992e+00,  9.2175e-02, -2.1713e-01,
          4.9406e-01,  7.3810e-01,  9.3650e-01, -1.1971e+00,  4.5122e-01,
         -3.8468e-01, -5.8609e-01, -2.9075e-01, -8.1114e-01, -7.2380e-01,
         -1.7126e-01,  4.5018e-01,  8.5658e-01, -1.0709e+00, -2.2684e-01,
         -2.0057e+00, -1.0212e+00,  7.6014e-01, -3.6282e-02, -1.5831e+00,
         -1.3225e+00, -1.8376e+00, -1.3217e-01,  6.3052e-01, -4.0304e-01,
          9.6190e-01,  5.0469e-01, -1.5259e-01, -6.0209e-01,  3.3894e-01,
          4.3941e-01,  7.5613e-01, -1.0534e+00, -2.0221e+00,  1.0982e-01,
          1.6380e-01,  7.4702e-01, -6.2162e-01, -6.1977e-01],
        [-9.7666e-01, -8.4439e-01,  2.2044e-01, -1.2790e-01,  4.6095e-01,
         -7.7718e-02, -1.4013e+00, -1.0678e+00, -1.1837e+00, -1.3390e+00,
         -1.0031e+00, -9.2532e-01,  9.5238e-01,  2.0641e+00, -7.0342e-01,
         -1.6927e-01,  2.4265e-01, -1.2714e+00,  2.3570e-01,  2.4192e-01,
         -1.0165e+00,  9.9284e-01,  7.0386e-01, -1.1791e-01, -3.8798e-01,
          1.8188e+00,  1.0037e-01, -7.3542e-01, -1.3672e+00,  3.8819e-01,
          7.9615e-02, -7.2596e-01,  8.9550e-01, -1.3222e+00, -7.5043e-02,
          4.7501e-01,  4.2576e-01, -3.1773e-01, -1.3501e+00,  2.7460e+00,
         -6.9808e-01,  8.1016e-01,  9.9220e-01, -1.3245e+00, -4.7727e-02,
          7.0704e-01, -1.5779e+00, -1.3452e-02,  1.0882e+00,  1.3080e+00,
         -9.9688e-01,  9.8518e-01, -1.2655e-01, -5.4067e-01,  6.2027e-01,
         -4.4712e-01, -7.4624e-01, -1.9487e-01, -4.5407e-01,  1.9106e+00,
          1.2317e+00, -2.8560e-01,  8.2261e-01, -3.9716e-01]])
inputs: tensor([[ 1.0630,  0.4133,  0.7127,  2.2289,  0.1333,  0.1413, -1.4622, -1.1813,
         -0.1615,  0.8503,  0.3987,  0.6569, -0.4402,  1.5677, -1.2769,  0.4945,
         -0.5781, -2.1179,  0.1678,  1.5826,  0.5769, -1.5247, -2.3197,  0.0719,
         -0.2574,  0.4484,  0.8160,  0.7439, -1.2002,  0.5555, -0.4376, -0.6453,
         -0.3481, -0.7546, -0.7141, -0.0943,  0.4040,  0.8965, -1.0673, -0.1628,
         -2.0671, -1.0260,  0.7635, -0.1970, -1.8412, -1.2678, -1.7341, -0.1910,
          0.6344, -0.4791,  1.1497,  0.6325, -0.2162, -0.6108,  0.3861,  0.4149,
          0.7981, -1.1813, -2.0958,  0.1714,  0.0998,  0.6465, -0.7248, -0.5659],
        [-0.9077, -0.8443,  0.2335, -0.1748,  0.4390, -0.2246, -1.5103, -1.1369,
         -1.2977, -1.4343, -1.1354, -0.9142,  0.9877,  2.0341, -0.9214, -0.0993,
          0.0240, -1.3833,  0.2849,  0.1289, -0.9474,  1.1400,  0.7164, -0.1924,
         -0.4889,  1.7300,  0.2621, -0.5594, -1.4595,  0.3052, -0.0371, -0.6762,
          0.8815, -1.3422,  0.1753,  0.3526,  0.4719, -0.3441, -1.2844,  2.7883,
         -0.8774,  0.8317,  1.0407, -1.2493,  0.0667,  0.8257, -1.6205,  0.0493,
          1.0501,  1.1321, -0.8197,  0.8934, -0.0787, -0.3373,  0.6824, -0.4390,
         -0.8446, -0.2162, -0.6235,  1.8854,  1.3046, -0.2252,  0.8203, -0.3948]])
candidate_state: tensor([[-7.1708e-02, -1.0644e-01,  1.4776e-02,  1.4256e-01,  8.5195e-02,
          7.6212e-01,  1.0973e-01, -1.4939e-01, -1.2956e-01,  6.6938e-02,
         -4.7296e-02, -1.4573e-01,  1.8970e-02,  3.8184e-02,  1.6275e-01,
         -1.4961e-01,  3.9326e-01, -1.6945e-01, -1.6943e-01,  9.7512e-01,
         -7.0231e-02, -5.2059e-02,  6.3627e-01, -1.2878e-01,  3.9630e-01,
          1.1057e-01,  3.3750e-01, -1.6958e-01, -1.2842e-01, -3.6273e-04,
         -1.5196e-01,  6.6480e-01,  2.0501e-02, -7.9533e-02, -1.6967e-01,
         -1.6408e-01, -5.5172e-02, -2.7561e-02, -1.6869e-01,  1.4223e-01,
          2.6886e-02,  8.7497e-02,  6.9520e-01,  1.1402e+00,  2.3604e-01,
          7.5630e-01,  4.7633e-02,  4.8826e-02, -7.1351e-02,  6.3993e-01,
          4.5453e-01,  3.5534e-01, -8.5900e-02, -1.6913e-01, -5.7680e-02,
          1.8619e-01, -1.7406e-02, -1.5894e-01, -5.4604e-02,  3.7102e-02,
         -3.4034e-02,  1.2001e-02,  2.5714e-01, -1.6925e-01],
        [-1.6189e-01,  6.8204e-02,  5.7701e-01, -1.2441e-01,  1.2049e+00,
          1.0059e+00, -1.0439e-01, -1.9051e-02, -1.0880e-01, -5.3995e-02,
          5.2656e-01, -5.8674e-03,  5.6181e-02,  5.4747e-02,  1.6295e-01,
         -1.2751e-01,  8.3889e-02, -1.6060e-01,  7.6375e-02,  7.7560e-01,
         -1.2478e-01,  5.0542e-01,  2.2820e-01,  1.9857e-01, -1.0594e-01,
          1.1439e-01,  1.7024e-01,  1.2954e-01, -1.6642e-01, -1.5380e-01,
         -1.6712e-01, -1.6969e-01, -1.0473e-01, -1.4626e-01,  4.9240e-01,
          1.9267e-01, -8.8082e-02, -3.6864e-02, -1.6141e-01,  3.0182e-01,
          4.5760e-01, -1.0399e-01,  2.1255e-01,  3.4774e-01,  4.5127e-01,
          7.6438e-02, -1.3764e-01,  9.8365e-02, -1.4118e-01,  1.9744e-01,
         -1.2116e-01,  5.1879e-04, -1.4606e-01, -1.5653e-01, -1.6968e-01,
          7.8881e-01, -9.7836e-02,  4.1194e-01, -1.2440e-01,  1.0860e-01,
          2.2524e-01,  4.6816e-01,  4.5255e-02, -1.0200e-01]])
new_state: tensor([[ 0.4690,  0.0870,  0.4346,  1.2338,  0.0582,  0.3707, -0.6301, -0.6003,
         -0.1052,  0.4347,  0.1302,  0.2950, -0.1693,  0.8332, -0.6340,  0.2030,
         -0.0706, -1.1482,  0.0093,  1.3262,  0.3433, -0.8186, -0.9260, -0.0151,
          0.0806,  0.3079,  0.5437,  0.3997, -0.6784,  0.2321, -0.2717,  0.0210,
         -0.1397, -0.4561, -0.4549, -0.1678,  0.2049,  0.4275, -0.6330, -0.0477,
         -1.0193, -0.4831,  0.7286,  0.5347, -0.7002, -0.3136, -0.9226, -0.0443,
          0.2899,  0.1031,  0.7157,  0.4322, -0.1202, -0.3920,  0.1465,  0.3165,
          0.3807, -0.6193, -1.0672,  0.0745,  0.0678,  0.3903, -0.1951, -0.4011],
        [-0.5581, -0.3755,  0.4036, -0.1261,  0.8432,  0.4790, -0.7350, -0.5290,
         -0.6314, -0.6788, -0.2172, -0.4529,  0.4919,  1.0317, -0.2583, -0.1478,
          0.1611, -0.7007,  0.1538,  0.5161, -0.5584,  0.7424,  0.4595,  0.0447,
         -0.2431,  0.9431,  0.1363, -0.2910, -0.7503,  0.1097, -0.0471, -0.4402,
          0.3816, -0.7180,  0.2165,  0.3300,  0.1618, -0.1734, -0.7394,  1.4902,
         -0.1043,  0.3405,  0.5916, -0.4654,  0.2086,  0.3831, -0.8379,  0.0440,
          0.4566,  0.7374, -0.5470,  0.4793, -0.1366, -0.3433,  0.2144,  0.1879,
         -0.4131,  0.1169, -0.2847,  0.9848,  0.7146,  0.1017,  0.4232, -0.2455]])
memory_gate: tensor([[0.5009],
        [0.5035]])
state: tensor([[ 9.7886e-01,  2.6937e-01,  8.3048e-01,  2.2628e+00,  3.2685e-02,
          1.5868e-03, -1.3277e+00, -1.0255e+00, -8.2196e-02,  7.8156e-01,
          2.9762e-01,  7.1066e-01, -3.4675e-01,  1.5828e+00, -1.3853e+00,
          5.3543e-01, -5.0805e-01, -2.0711e+00,  1.7787e-01,  1.6572e+00,
          7.3316e-01, -1.5413e+00, -2.3992e+00,  9.2175e-02, -2.1713e-01,
          4.9406e-01,  7.3810e-01,  9.3650e-01, -1.1971e+00,  4.5122e-01,
         -3.8468e-01, -5.8609e-01, -2.9075e-01, -8.1114e-01, -7.2380e-01,
         -1.7126e-01,  4.5018e-01,  8.5658e-01, -1.0709e+00, -2.2684e-01,
         -2.0057e+00, -1.0212e+00,  7.6014e-01, -3.6282e-02, -1.5831e+00,
         -1.3225e+00, -1.8376e+00, -1.3217e-01,  6.3052e-01, -4.0304e-01,
          9.6190e-01,  5.0469e-01, -1.5259e-01, -6.0209e-01,  3.3894e-01,
          4.3941e-01,  7.5613e-01, -1.0534e+00, -2.0221e+00,  1.0982e-01,
          1.6380e-01,  7.4702e-01, -6.2162e-01, -6.1977e-01],
        [-9.7666e-01, -8.4439e-01,  2.2044e-01, -1.2790e-01,  4.6095e-01,
         -7.7718e-02, -1.4013e+00, -1.0678e+00, -1.1837e+00, -1.3390e+00,
         -1.0031e+00, -9.2532e-01,  9.5238e-01,  2.0641e+00, -7.0342e-01,
         -1.6927e-01,  2.4265e-01, -1.2714e+00,  2.3570e-01,  2.4192e-01,
         -1.0165e+00,  9.9284e-01,  7.0386e-01, -1.1791e-01, -3.8798e-01,
          1.8188e+00,  1.0037e-01, -7.3542e-01, -1.3672e+00,  3.8819e-01,
          7.9615e-02, -7.2596e-01,  8.9550e-01, -1.3222e+00, -7.5043e-02,
          4.7501e-01,  4.2576e-01, -3.1773e-01, -1.3501e+00,  2.7460e+00,
         -6.9808e-01,  8.1016e-01,  9.9220e-01, -1.3245e+00, -4.7727e-02,
          7.0704e-01, -1.5779e+00, -1.3452e-02,  1.0882e+00,  1.3080e+00,
         -9.9688e-01,  9.8518e-01, -1.2655e-01, -5.4067e-01,  6.2027e-01,
         -4.4712e-01, -7.4624e-01, -1.9487e-01, -4.5407e-01,  1.9106e+00,
          1.2317e+00, -2.8560e-01,  8.2261e-01, -3.9716e-01]])
inputs: tensor([[-0.4545,  0.0629,  1.0320, -1.0407,  0.2309,  0.7773,  0.7965,  0.7798,
         -0.3210, -0.1509, -0.2347, -0.1400, -0.4872,  1.2184,  0.1397, -0.0495,
         -0.9334,  0.8774, -0.5297,  0.8783, -0.3679, -0.2467, -0.4604, -1.6719,
         -2.0018, -2.1077,  0.7314, -0.8909, -0.9750, -0.7980,  1.4560,  1.6964,
         -0.6227,  1.0531,  0.7154, -0.8636,  0.0130,  1.7707, -1.5209,  1.7544,
         -0.3343,  0.7799, -1.9300, -0.9240, -1.5951, -0.1909,  0.1878, -0.0188,
          0.0677, -0.5072, -0.8041,  0.9733,  0.3907,  0.0583, -0.9149,  1.2418,
          0.5750,  0.1434, -0.7687,  0.4812,  0.0572, -1.2298,  0.1458, -0.1002],
        [ 0.3886, -1.1736, -0.2050,  1.0496,  0.6228,  0.0857, -0.1007,  2.5299,
          1.0769,  0.8679, -1.6551, -2.2297,  0.8465,  0.3321, -0.6438, -0.4564,
          0.6579,  0.4484, -1.6544,  1.5634, -0.7049,  1.3821, -0.5204, -1.0421,
         -1.0294,  1.5696,  0.3368, -0.1377,  1.7551, -0.9779, -0.3441, -0.8534,
          0.1107,  0.1597,  1.5520,  0.7153, -1.7254,  0.7222, -0.9744, -1.5926,
          0.7059,  0.6536, -0.4157,  0.7627, -0.0221, -0.6498,  0.2804, -1.1372,
          0.0358,  0.8799,  1.3005,  1.2305,  1.1962,  1.1446, -0.0655,  0.5228,
         -0.4144, -1.7397,  0.3523,  1.4139,  0.4563,  0.8803,  0.6027, -0.1013]])
candidate_state: tensor([[ 0.4424, -0.0995, -0.0692, -0.1687,  0.2021,  1.0941, -0.0073, -0.1632,
          0.3805, -0.1461,  0.1418,  0.5197,  0.5235, -0.1207, -0.0086,  0.3020,
         -0.1035, -0.1668, -0.1657,  0.1912, -0.1601, -0.1451,  0.2971, -0.1038,
         -0.0640,  0.4576, -0.1360, -0.1239,  0.0214, -0.1620, -0.0479,  0.0058,
          0.1284, -0.1181,  0.3370, -0.0950,  0.0099, -0.1440, -0.1330, -0.1298,
         -0.1399, -0.1259, -0.0448,  0.6356,  0.2909,  0.2097,  0.1255,  0.0614,
         -0.0859, -0.1567, -0.1600,  0.0297,  0.0503,  0.1191, -0.1678, -0.1040,
          0.7967,  0.7131,  0.0488, -0.0608, -0.0674,  0.1636, -0.1416,  0.8895],
        [-0.0191, -0.1513, -0.1690,  0.4176, -0.1631, -0.1524,  0.2683, -0.1487,
          0.7425, -0.1665, -0.1430, -0.0609,  0.7537, -0.1583, -0.1084, -0.1554,
         -0.1686, -0.1681, -0.1084,  0.7482,  0.0590, -0.1694,  0.5458, -0.1587,
          0.4037, -0.1685,  1.4029,  0.2742,  0.9302,  0.5368,  0.7571, -0.1038,
          0.0589, -0.1108, -0.1202,  0.3235,  0.1301, -0.0610, -0.0326, -0.1305,
          0.1673, -0.1219,  0.5563,  0.9856, -0.0144, -0.1336,  0.7396, -0.1675,
          0.1103,  0.1504, -0.1107,  0.1102,  0.4089, -0.0955,  0.3307, -0.1652,
         -0.1282,  0.0040,  0.4915, -0.1687,  0.0701, -0.1646, -0.0699, -0.1248]])
new_state: tensor([[ 0.7111,  0.0852,  0.3814,  1.0492,  0.1172,  0.5469, -0.6687, -0.5951,
          0.1487,  0.3185,  0.2199,  0.6153,  0.0876,  0.7325, -0.6981,  0.4189,
         -0.3061, -1.1206,  0.0064,  0.9255,  0.2873, -0.8444, -1.0534, -0.0056,
         -0.1407,  0.4759,  0.3018,  0.4072, -0.5889,  0.1451, -0.2166, -0.2907,
         -0.0816, -0.4652, -0.1943, -0.1332,  0.2304,  0.3572, -0.6028, -0.1784,
         -1.0745, -0.5743,  0.3584,  0.2991, -0.6477, -0.5577, -0.8577, -0.0355,
          0.2729, -0.2801,  0.4019,  0.2676, -0.0513, -0.2421,  0.0860,  0.1682,
          0.7764, -0.1717, -0.9885,  0.0247,  0.0484,  0.4558, -0.3820,  0.1336],
        [-0.5012, -0.5003,  0.0271,  0.1430,  0.1511, -0.1148, -0.5723, -0.6115,
         -0.2273, -0.7569, -0.5761, -0.4961,  0.8537,  0.9606, -0.4080, -0.1624,
          0.0385, -0.7236,  0.0648,  0.4933, -0.4825,  0.4158,  0.6254, -0.1382,
          0.0051,  0.8321,  0.7471, -0.2342, -0.2266,  0.4620,  0.4160, -0.4171,
          0.4801, -0.7207, -0.0975,  0.3998,  0.2790, -0.1903, -0.6959,  1.3178,
         -0.2684,  0.3474,  0.7758, -0.1775, -0.0312,  0.2896, -0.4272, -0.0899,
          0.6027,  0.7332, -0.5569,  0.5507,  0.1393, -0.3197,  0.4765, -0.3071,
         -0.4394, -0.0961,  0.0154,  0.8782,  0.6550, -0.2255,  0.3795, -0.2619]])
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x7e13f41f2230>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_phi_metric_computation(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Create sample inputs
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Initialize parameters
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-1.8037e-01, -6.2268e-01,  1.5333e+00,  1.8292e-01,  9.0753e-02,
           1.3011e-01, -8.3961e-02, -8.3455...e-01,
           1.0585e-01,  1.6349e-01,  8.4526e-02, -1.3541e+00, -1.7722e+00,
          -1.8435e+00,  2.6966e+00]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x7e13f41f3cd0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_information_flow(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.zeros(batch_size, num_modules, input_dim, device=device)  # ensure shape matches the model
    
        # Test with and without dropout
        integration_module.train()
>       output1, _ = integration_module(inputs)

tests/unit/memory/test_integration.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestInformationIntegration.test_entropy_calculations _____________

self = <test_integration.TestInformationIntegration object at 0x7e13f41f39d0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_entropy_calculations(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = torch.ones(batch_size, num_modules, input_dim, device=device)
        integration_module.eval()
        with torch.no_grad():
>           _, phi_uniform = integration_module(uniform_input)

tests/unit/memory/test_integration.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1....., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x7e13f41f3d90>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_integration(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Process through integration
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-3.4776e-01,  7.1394e-01,  5.8023e-01, -2.0446e+00,  3.4124e-01,
          -2.3266e+00,  6.1248e-01,  1.1508...e-01,
           6.2403e-01, -9.0151e-04, -7.7051e-01,  6.6301e-01,  1.4775e-01,
          -1.4349e-01,  1.5482e+00]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestStateManagement.test_state_value_estimation ________________

self = <test_consciousness_state_management.TestStateManagement object at 0x7e13f4056920>
state_manager = ConsciousnessStateManager(), batch_size = 2, hidden_dim = 64

    def test_state_value_estimation(self, state_manager, batch_size, hidden_dim):
        """Test state value estimation."""
        consciousness_state = torch.randn(batch_size, hidden_dim)
        integrated_output = torch.randn(batch_size, hidden_dim)
    
        # Test value estimation consistency
        _, metrics1 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
        _, metrics2 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
    
        # Same input should give same value estimate
>       assert torch.allclose(metrics1['state_value'], metrics2['state_value'], rtol=1e-5)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7e142d0678c0>(tensor([[ 0.4191],\n        [-0.2081]], grad_fn=<AddmmBackward0>), tensor([[-0.3795],\n        [ 0.4519]], grad_fn=<AddmmBackward0>), rtol=1e-05)
E        +    where <built-in method allclose of type object at 0x7e142d0678c0> = torch.allclose

tests/unit/state/test_consciousness_state_management.py:100: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4795],
        [0.5179]], grad_fn=<SigmoidBackward0>)
state: tensor([[-0.4940,  0.4784, -0.9295,  1.4067, -1.7085, -0.7998,  0.0196,  1.2693,
         -1.4851, -1.1657, -0.1551, -0.7369,  2.2909,  0.5737, -0.0620, -1.5723,
         -0.1659, -2.0173,  0.6769, -0.4635,  0.1812,  1.3763,  0.4292, -0.8313,
          0.9989,  0.8951, -0.3612,  0.0791,  0.5483,  0.5783,  0.3787, -1.4868,
          1.7853, -0.3403, -0.2890, -1.2615,  1.7665,  0.3493, -0.1138,  0.3441,
         -0.8403, -0.7285,  0.6316,  1.5626,  0.0522,  0.9090, -0.6318,  1.3370,
          1.0443, -1.0886, -0.0931, -0.0080,  1.3693, -1.3507, -1.9816,  0.4727,
         -1.4291,  0.8439, -0.4262, -1.0504,  0.6151, -0.1482, -0.5976, -0.2193],
        [-0.0815, -0.6241,  0.5460,  0.6309,  0.6275,  1.1084,  0.2940,  0.6949,
         -0.0522, -0.4332,  0.9967,  0.7714, -0.4244,  2.3659,  1.6521, -0.2006,
          0.6946,  1.1259, -0.1994, -0.5958,  1.7325,  0.3104, -1.8889,  0.9515,
          0.4065, -0.2948, -1.6039, -0.0592, -1.2180,  2.1500,  0.4340, -0.9884,
         -1.2104,  1.5162,  0.3994, -0.6306, -0.3275, -1.2177, -1.8089, -0.5216,
         -0.3411,  0.7843,  0.4763,  0.9764,  0.2752,  1.5664,  0.2778,  1.4289,
          1.4541, -1.2868,  1.5016,  0.0042,  0.7538,  0.6215,  1.2956, -1.8578,
         -0.7462, -0.3370, -0.0329,  0.6048,  1.9176,  0.4684,  0.5744, -1.8907]])
inputs: tensor([[-0.6380, -1.3155,  0.4525,  1.5370,  1.0628,  0.4237,  0.1735,  0.4740,
         -1.7451,  2.2172,  2.2443,  0.1623, -2.3763, -0.9294, -0.4473,  0.2798,
         -1.0048,  1.0624, -0.2218, -0.4094,  0.7561, -0.0775, -0.2704, -2.1350,
          1.1796, -0.5667,  1.5075, -0.7689,  1.0891,  0.1051,  0.0622, -0.4795,
          0.1793,  0.3814,  0.7187, -1.4758,  1.1222,  0.8378,  0.2079,  0.1015,
         -2.9463, -0.3960,  0.6809,  1.5740, -0.5612,  1.0543, -1.3135,  1.5570,
          2.8870,  1.0451, -1.2044, -0.3028, -1.7179,  0.2053, -0.3454, -0.4083,
         -0.2162,  0.4087, -0.5263, -1.3603,  0.1007,  0.1962, -1.1400,  1.5063],
        [-0.7550, -0.8222,  0.5451, -0.8227, -0.7718,  0.2517,  1.8986,  0.2307,
         -0.1750,  0.0141,  0.2116,  1.2091,  0.0955, -1.6022,  3.0795,  0.5168,
         -0.6630,  1.3358, -0.4731,  0.8753, -0.7916, -1.1074, -0.3575,  0.9741,
          1.1230,  0.9722,  0.7169,  0.8343,  0.3929, -0.1129,  0.0647, -0.5655,
          0.9121, -0.1326,  0.7155,  0.1623,  0.6489,  1.9535,  1.1419,  2.5714,
         -0.5817, -1.1428,  0.7223,  0.6978,  1.6610,  1.2770,  0.1195, -0.0085,
          0.4017,  0.7737, -1.2258, -1.7283, -1.8460,  2.2270, -0.0508,  0.0111,
         -1.0781, -0.4405, -0.8798,  0.2016,  0.2711, -0.3494, -1.7581,  2.0544]])
candidate_state: tensor([[-1.3029e-01,  4.5074e-01, -1.0495e-01,  7.6144e-01, -1.2155e-01,
          4.3383e-01,  1.1614e+00, -1.4430e-01,  1.4095e-01,  6.3132e-01,
         -3.8393e-02, -1.6962e-01, -1.6431e-01,  2.3666e-01, -1.5263e-01,
         -7.2685e-03, -8.4674e-02,  4.3503e-01, -1.6372e-01, -1.6675e-01,
         -1.6945e-01, -1.4553e-02,  9.4742e-02,  1.6040e-01,  1.4211e+00,
         -1.1801e-01, -6.7394e-02, -1.6513e-01, -1.5200e-01,  3.0808e-01,
          4.3514e-01,  1.8530e-01,  1.2196e-01,  6.7170e-02, -1.6455e-01,
          1.1745e-02, -1.6749e-01, -6.7966e-02, -3.5481e-02, -9.8573e-02,
         -1.6194e-01, -1.5819e-01,  5.0151e-01,  1.0106e-01, -1.5070e-01,
         -1.4777e-01, -1.6841e-01, -6.8925e-02,  3.4487e-01,  7.7519e-03,
         -1.6995e-01,  5.9136e-01, -3.5967e-02, -5.8066e-03,  1.2266e+00,
         -1.6332e-01, -1.5537e-01, -1.6258e-01, -1.0501e-01, -9.7049e-02,
         -5.3318e-02,  5.3143e-01, -1.5276e-01,  4.2655e-01],
        [-9.3342e-03, -8.6233e-02, -1.1588e-01, -1.6547e-01, -1.6996e-01,
         -1.4097e-01, -6.8733e-02, -1.0385e-01,  3.6005e-01,  2.0663e-01,
          4.1497e-01, -1.6424e-02, -1.0335e-01,  3.0370e-01,  1.5807e-01,
          2.3938e-02,  3.7417e-01,  2.2107e-01,  3.6336e-01,  4.6641e-01,
          3.6403e-01,  7.3495e-01,  1.7652e-01, -1.6287e-01, -9.9499e-02,
         -1.2205e-01, -1.2308e-01,  1.1223e+00,  2.2190e-01, -7.2785e-02,
         -1.1171e-01, -1.6070e-01, -8.7721e-02,  5.6653e-01, -1.3539e-01,
         -2.3420e-03,  2.3998e-01,  1.4140e-02,  4.5414e-04, -9.3469e-02,
         -1.4686e-01, -1.5092e-01, -8.8164e-03, -4.7548e-02,  4.4877e-01,
          1.4627e-02,  1.4766e-01,  8.6242e-02,  2.7382e-01, -1.6996e-01,
          1.7038e-01,  6.9652e-01, -1.5504e-01,  2.0408e-01,  6.4750e-01,
          3.0199e-01, -1.5336e-01,  7.3563e-01,  4.3218e-03,  3.8144e-01,
         -1.5208e-01,  7.8397e-01, -5.0045e-03, -7.1302e-02]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[-3.0469e-01,  4.6403e-01, -5.0026e-01,  1.0708e+00, -8.8241e-01,
         -1.5763e-01,  6.1395e-01,  5.3348e-01, -6.3869e-01, -2.3027e-01,
         -9.4332e-02, -4.4161e-01,  1.0129e+00,  3.9823e-01, -1.0918e-01,
         -7.5762e-01, -1.2360e-01, -7.4075e-01,  2.3931e-01, -3.0904e-01,
         -1.3385e-03,  6.5232e-01,  2.5509e-01, -3.1509e-01,  1.2186e+00,
          3.6771e-01, -2.0826e-01, -4.8055e-02,  1.8378e-01,  4.3765e-01,
          4.0807e-01, -6.1641e-01,  9.1944e-01, -1.2822e-01, -2.2422e-01,
         -5.9873e-01,  7.5979e-01,  1.3210e-01, -7.3040e-02,  1.1368e-01,
         -4.8720e-01, -4.3163e-01,  5.6386e-01,  8.0182e-01, -5.3402e-02,
          3.5893e-01, -3.9059e-01,  6.0516e-01,  6.8023e-01, -5.1791e-01,
         -1.3311e-01,  3.0398e-01,  6.3778e-01, -6.5062e-01, -3.1158e-01,
          1.4161e-01, -7.6609e-01,  3.1998e-01, -2.5898e-01, -5.5413e-01,
          2.6717e-01,  2.0558e-01, -3.6604e-01,  1.1691e-01],
        [-4.6730e-02, -3.6478e-01,  2.2690e-01,  2.4695e-01,  2.4303e-01,
          5.0606e-01,  1.1912e-01,  3.0983e-01,  1.4658e-01, -1.2473e-01,
          7.1622e-01,  3.9159e-01, -2.6963e-01,  1.3717e+00,  9.3182e-01,
         -9.2321e-02,  5.4012e-01,  6.8966e-01,  7.1922e-02, -8.3710e-02,
          1.0727e+00,  5.1509e-01, -8.9314e-01,  4.1425e-01,  1.6256e-01,
         -2.1152e-01, -8.8996e-01,  5.1042e-01, -5.2380e-01,  1.0784e+00,
          1.7088e-01, -5.8937e-01, -6.6913e-01,  1.0584e+00,  1.4158e-01,
         -3.2769e-01, -5.3931e-02, -6.2383e-01, -9.3659e-01, -3.1520e-01,
         -2.4746e-01,  3.3341e-01,  2.4241e-01,  4.8273e-01,  3.5887e-01,
          8.1828e-01,  2.1505e-01,  7.8160e-01,  8.8506e-01, -7.4837e-01,
          8.5981e-01,  3.3797e-01,  3.1563e-01,  4.2024e-01,  9.8311e-01,
         -8.1653e-01, -4.6039e-01,  1.8011e-01, -1.4944e-02,  4.9710e-01,
          9.1978e-01,  6.2054e-01,  2.9506e-01, -1.0136e+00]],
       grad_fn=<AddBackward0>)
memory_gate: tensor([[0.5038],
        [0.5288]], grad_fn=<SigmoidBackward0>)
state: tensor([[-0.4940,  0.4784, -0.9295,  1.4067, -1.7085, -0.7998,  0.0196,  1.2693,
         -1.4851, -1.1657, -0.1551, -0.7369,  2.2909,  0.5737, -0.0620, -1.5723,
         -0.1659, -2.0173,  0.6769, -0.4635,  0.1812,  1.3763,  0.4292, -0.8313,
          0.9989,  0.8951, -0.3612,  0.0791,  0.5483,  0.5783,  0.3787, -1.4868,
          1.7853, -0.3403, -0.2890, -1.2615,  1.7665,  0.3493, -0.1138,  0.3441,
         -0.8403, -0.7285,  0.6316,  1.5626,  0.0522,  0.9090, -0.6318,  1.3370,
          1.0443, -1.0886, -0.0931, -0.0080,  1.3693, -1.3507, -1.9816,  0.4727,
         -1.4291,  0.8439, -0.4262, -1.0504,  0.6151, -0.1482, -0.5976, -0.2193],
        [-0.0815, -0.6241,  0.5460,  0.6309,  0.6275,  1.1084,  0.2940,  0.6949,
         -0.0522, -0.4332,  0.9967,  0.7714, -0.4244,  2.3659,  1.6521, -0.2006,
          0.6946,  1.1259, -0.1994, -0.5958,  1.7325,  0.3104, -1.8889,  0.9515,
          0.4065, -0.2948, -1.6039, -0.0592, -1.2180,  2.1500,  0.4340, -0.9884,
         -1.2104,  1.5162,  0.3994, -0.6306, -0.3275, -1.2177, -1.8089, -0.5216,
         -0.3411,  0.7843,  0.4763,  0.9764,  0.2752,  1.5664,  0.2778,  1.4289,
          1.4541, -1.2868,  1.5016,  0.0042,  0.7538,  0.6215,  1.2956, -1.8578,
         -0.7462, -0.3370, -0.0329,  0.6048,  1.9176,  0.4684,  0.5744, -1.8907]])
inputs: tensor([[-0.6380, -1.3155,  0.4525,  1.5370,  1.0628,  0.4237,  0.1735,  0.4740,
         -1.7451,  2.2172,  2.2443,  0.1623, -2.3763, -0.9294, -0.4473,  0.2798,
         -1.0048,  1.0624, -0.2218, -0.4094,  0.7561, -0.0775, -0.2704, -2.1350,
          1.1796, -0.5667,  1.5075, -0.7689,  1.0891,  0.1051,  0.0622, -0.4795,
          0.1793,  0.3814,  0.7187, -1.4758,  1.1222,  0.8378,  0.2079,  0.1015,
         -2.9463, -0.3960,  0.6809,  1.5740, -0.5612,  1.0543, -1.3135,  1.5570,
          2.8870,  1.0451, -1.2044, -0.3028, -1.7179,  0.2053, -0.3454, -0.4083,
         -0.2162,  0.4087, -0.5263, -1.3603,  0.1007,  0.1962, -1.1400,  1.5063],
        [-0.7550, -0.8222,  0.5451, -0.8227, -0.7718,  0.2517,  1.8986,  0.2307,
         -0.1750,  0.0141,  0.2116,  1.2091,  0.0955, -1.6022,  3.0795,  0.5168,
         -0.6630,  1.3358, -0.4731,  0.8753, -0.7916, -1.1074, -0.3575,  0.9741,
          1.1230,  0.9722,  0.7169,  0.8343,  0.3929, -0.1129,  0.0647, -0.5655,
          0.9121, -0.1326,  0.7155,  0.1623,  0.6489,  1.9535,  1.1419,  2.5714,
         -0.5817, -1.1428,  0.7223,  0.6978,  1.6610,  1.2770,  0.1195, -0.0085,
          0.4017,  0.7737, -1.2258, -1.7283, -1.8460,  2.2270, -0.0508,  0.0111,
         -1.0781, -0.4405, -0.8798,  0.2016,  0.2711, -0.3494, -1.7581,  2.0544]])
candidate_state: tensor([[-0.1665, -0.0178,  1.0663, -0.0165,  0.1016,  0.2549, -0.1626,  0.2130,
         -0.1425, -0.1393, -0.1147,  0.3887, -0.0926, -0.1638, -0.1068, -0.1166,
          0.1606, -0.1437,  0.0899,  0.3328,  0.7445,  0.1272, -0.1437, -0.1450,
          1.0480,  0.8636,  0.1229,  0.0468,  0.1737, -0.0288, -0.1675, -0.1700,
         -0.1621,  0.1874,  0.1338, -0.1691, -0.1605,  0.2309,  0.1325,  0.1255,
         -0.1332,  0.0599,  0.0277,  0.1252, -0.1533,  0.6779,  0.2244, -0.1121,
          1.0918, -0.1272, -0.1572, -0.0439,  1.3949, -0.1591,  0.0422, -0.0451,
         -0.0872, -0.1661,  0.3617,  0.0650, -0.1623, -0.1340,  0.0499, -0.0225],
        [-0.1658,  0.8885,  0.3366, -0.1176, -0.1699,  0.0540, -0.1500,  0.4516,
          0.4639, -0.1659, -0.0685,  0.8655,  0.6049,  0.9448,  0.0920, -0.1641,
         -0.1299,  0.0907,  0.4092,  0.0568,  0.4180, -0.0975, -0.1448,  0.1933,
          0.1959,  0.3634,  0.0216,  0.0076,  0.0557,  0.0218,  0.1132, -0.0883,
         -0.1623, -0.0508, -0.1584, -0.0094,  0.8315, -0.1586, -0.1480, -0.1686,
          0.4394, -0.0535, -0.1526,  0.5841,  0.0215,  0.3139, -0.0407,  0.5230,
          0.0880, -0.1613, -0.1384, -0.1664,  0.3452, -0.0646, -0.0973, -0.1650,
          0.3267, -0.0960, -0.1150,  0.2545, -0.0488, -0.1101, -0.0939,  0.0510]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[-0.3315,  0.2322,  0.0608,  0.7006, -0.8104, -0.2765, -0.0708,  0.7452,
         -0.8190, -0.6564, -0.1350, -0.1784,  1.1083,  0.2078, -0.0842, -0.8500,
         -0.0039, -1.0877,  0.3857, -0.0684,  0.4607,  0.7565,  0.1449, -0.4908,
          1.0232,  0.8795, -0.1210,  0.0631,  0.3625,  0.2771,  0.1077, -0.8334,
          0.8191, -0.0785, -0.0792, -0.7195,  0.8104,  0.2906,  0.0084,  0.2357,
         -0.4895, -0.3373,  0.3320,  0.8494, -0.0498,  0.7943, -0.2070,  0.6180,
          1.0679, -0.6116, -0.1249, -0.0258,  1.3820, -0.7594, -0.9775,  0.2158,
         -0.7633,  0.3428, -0.0352, -0.4970,  0.2294, -0.1412, -0.2763, -0.1216],
        [-0.1212,  0.0887,  0.4473,  0.2782,  0.2517,  0.6116,  0.0848,  0.5803,
          0.1910, -0.3073,  0.4948,  0.8158,  0.0606,  1.6963,  0.9170, -0.1834,
          0.3061,  0.6381,  0.0874, -0.2883,  1.1131,  0.1182, -1.0671,  0.5942,
          0.3073,  0.0153, -0.8379, -0.0277, -0.6178,  1.1472,  0.2828, -0.5643,
         -0.7165,  0.7778,  0.1366, -0.3379,  0.2186, -0.7187, -1.0263, -0.3553,
          0.0267,  0.3895,  0.1799,  0.7915,  0.1556,  0.9762,  0.1277,  1.0021,
          0.8104, -0.7565,  0.7288, -0.0762,  0.5612,  0.2982,  0.6392, -1.0601,
         -0.2406, -0.2235, -0.0716,  0.4397,  0.9910,  0.1958,  0.2595, -0.9758]],
       grad_fn=<AddBackward0>)
=============================== warnings summary ===============================
tests/unit/integration/test_state_management.py: 4 warnings
tests/unit/state/test_consciousness_state_management.py: 6 warnings
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py: 4 warnings
tests/unit/state/test_consciousness_state_management.py: 6 warnings
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = torch.tensor(inputs, dtype=torch.float32)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_environment.py::EnvironmentTests::test_core_imports - Runti...
FAILED tests/test_environment.py::EnvironmentTests::test_framework_versions
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_scaled_dot_product
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_attention_mask
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_consciousness_broadcasting
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_global_workspace_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_value_estimation
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_config
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_gru_state_updates
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_sequence_processing
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_context_aware_gating
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_information_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_retention
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_state_updates
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_reset_gate
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_sequence_processing
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_memory_retention
======= 13 failed, 15 passed, 2 skipped, 20 warnings, 19 errors in 1.07s =======
